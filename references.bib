@Article{levelt_theory_1999,
  title = {A Theory of Lexical Access in Speech Production},
  volume = {22},
  issn = {0140-525X},
  abstract = {Preparing words in speech production is normally a fast and accurate process. We generate them two or three per second in fluent conversation; and overtly naming a clear picture of an object can easily be initiated within 600 msec after picture onset. The underlying process, however, is exceedingly complex. The theory reviewed in this target article analyzes this process as staged and feed-forward. After a first stage of conceptual preparation, word generation proceeds through lexical selection, morphological and phonological encoding, phonetic encoding, and articulation itself. In addition, the speaker exerts some degree of output control, by monitoring of self-produced internal and overt speech. The core of the theory, ranging from lexical selection to the initiation of phonetic encoding, is captured in a computational model, called WEAVER++. Both the theory and the computational model have been developed in interaction with reaction time experiments, particularly in picture naming or related word production paradigms, with the aim of accounting for the real-time processing in normal word production. A comprehensive review of theory, model, and experiments is presented. The model can handle some of the main observations in the domain of speech errors (the major empirical domain for most other theories of lexical access), and the theory opens new ways of approaching the cerebral organization of speech production by way of high-temporal-resolution imaging.},
  language = {eng},
  number = {1},
  journal = {The Behavioral and Brain Sciences},
  author = {W. J. Levelt and A. Roelofs and A. S. Meyer},
  month = {feb},
  year = {1999},
  keywords = {Computer Simulation,Humans,Language Development,Linguistics,Magnetoencephalography,Memory,Models; Psychological,Phonetics,Reaction Time,Software,Speech,Speech Production Measurement},
  pages = {1-38; discussion 38-75},
  pmid = {11301520},
}
@Article{levelt_monitoring_1983,
  title = {Monitoring and Self-Repair in Speech},
  volume = {14},
  issn = {0010-0277},
  abstract = {Making a self-repair in speech typically proceeds in three phases. The first phase involves the monitoring of one's own speech and the interruption of the flow of speech when trouble is detected. From an analysis of 959 spontaneous self-repairs it appears that interrupting follows detection promptly, with the exception that correct words tend to be completed. Another finding is that detection of trouble improves towards the end of constituents. The second phase is characterized by hesitation, pausing, but especially the use of so-called editing terms. Which editing term is used depends on the nature of the speech trouble in a rather regular fashion: Speech errors induce other editing terms than words that are merely inappropriate, and trouble which is detected quickly by the speaker is preferably signalled by the use of `uh'. The third phase consists of making the repair proper. The linguistic well-formedness of a repair is not dependent on the speaker's respecting the integrity of constituents, but on the structural relation between original utterance and repair. A bi-conditional well-formedness rule links this relation to a corresponding relation between the conjuncts of a coordination. It is suggested that a similar relation holds also between question and answer. In all three cases the speaker respects certain structural commitments derived from an original utterance. It was finally shown that the editing term plus the first word of the repair proper almost always contain sufficient information for the listener to decide how the repair should be related to the original utterance. Speakers almost never produce misleading information in this respect. It is argued that speakers have little or no access to their speech production process; self-monitoring is probably based on parsing one's own inner or overt speech.
R{\'e}sum{\'e}
L'auto-correction dans le discours se fait typiquement en trois temps. Dans un premier temps, le locuteur contr{\^o}le sa propre parole et l'interrompt lorsqu'il rencontre un probl{\`e}me. Une analyse de 959 corrections spontan{\'e}es indique que l'interruption suit de tr{\`e}s pr{\`e}s la perception du probl{\`e}me, {\`a} l'exception pr{\`e}s que le locuteur a tendance {\`a} finir les mots corrects. Les r{\'e}sultats de cette analyse indiquent d'autre part que la perception du probl{\`e}me s'am{\'e}liore vers la fin des constituants. Le deuxi{\`e}me temps se caract{\'e}rise par des h{\'e}sitations, des pauses, mais surtout par l'utilisation de ce qu' on peut appeler les `commentaires r{\'e}dactionnels'. Ceux-ci sont li{\'e}s de fa{\c c}on suffisamment r{\'e}gulaire {\`a} la correction particuli{\`e}re qui est faite: ils sont diff{\'e}rents lorsqu'il s'agit d'une v{\'e}ritable erreur et lorsqu'il s'agit simplement d'une mauvaise tournure de phrase. La pr{\'e}sence imm{\'e}diate du probl{\`e}me est signal{\'e}e par l'utilisation de `uh'. Dans le troisi{\`e}me temps a lieu la correction elle-m{\^e}me. La bonne-formation des corrections ne d{\'e}pend pas de ce que le locuteur respecte l'int{\'e}grit{\'e} des constituants, mais plut{\^o}t de la relation structurelle qui existe entre le premier {\'e}nonc{\'e} et la correction. Cette relation est li{\'e}e {\`a} la relation correspondante entre les {\'e}l{\'e}ments conjoints d'une coordination par une r{\`e}gle de bonne formation bi-conditionnelle. On peut {\'e}galement sugg{\'e}rer qu'il existe une relation semblable entre questions et r{\'e}ponses. Dans ces trois cas, le locuteur respecte les contraintes structurelles de son premier {\'e}nonc{\'e}. Enfin, l'analyse d{\'e}montre que l'ensemble form{\'e} par le `commentaire r{\'e}dactionnel' et le premier mot de la correction elle-m{\^e}me contient presque toujours des {\'e}l{\'e}ments d'information permettant {\`a} l'interlocuteur de d{\'e}cider comment il faut relier la correction au premier {\'e}nonc{\'e}. De ce point de vue, les locuteurs ne produisent presque jamais d'{\'e}nonc{\'e}s qui pourraient induire leur interlocuteur en erreur. Ces r{\'e}sultats indiquent que le locuteur a peu ou pas du tout d'acc{\`e}s au processus de production d'{\'e}nonc{\'e}s; l'auto-contr{\^o}le se fait plut{\^o}t {\`a} partir de la compr{\'e}hension de sa propre parole int{\'e}rieure ou ext{\'e}rieure.},
  number = {1},
  journal = {Cognition},
  doi = {10.1016/0010-0277(83)90026-4},
  author = {Willem J. M. Levelt},
  month = {jul},
  year = {1983},
  pages = {41-104},
  file = {/Users/Ladislas/Zotero/storage/L9KEAAWL/Levelt - 1983 - Monitoring and self-repair in speech.pdf;/Users/Ladislas/Zotero/storage/GPQ9KJTX/0010027783900264.html},
}

@Article{hartsuiker_error_2001,
  title = {Error Monitoring in Speech Production: A Computational Test of the Perceptual Loop Theory},
  volume = {42},
  issn = {0010-0285},
  shorttitle = {Error Monitoring in Speech Production},
  abstract = {A theory of speech monitoring, proposed by Levelt (1983), assumes that the quality of one's speech is checked by the speech comprehension system. This system inspects one's own overt speech but would also inspect an inner speech plan ({"}the inner loop{"}). We have elaborated and tested this theory by way of formalizing it as a computational model. This model includes a new proposal concerning the timing relation between planning the interruption and the repair: the proposal that these two processes are performed in parallel. We attempted to simulate empirical data about the distribution of error-to-cutoff and cutoff-to-repair intervals and the effect of speech rate on these intervals (these intervals are shorter with faster speech). The main questions were (1) Is an inner monitor that utilizes the speech perception system fast enough to simulate the timing data? (2) Can the model account for the effects of speech rate on these intervals? We conclude that including an inner loop through the speech comprehension system generates predictions that fit the empirical data. The effects of speed can be accounted for, given our proposal about the time course of planning interruption and repair. A novel prediction is that the error-to-cutoff interval decreases with increasing position in the phrase.},
  language = {eng},
  number = {2},
  journal = {Cognitive Psychology},
  doi = {10.1006/cogp.2000.0744},
  author = {R. J. Hartsuiker and H. H. Kolk},
  month = {mar},
  year = {2001},
  keywords = {Humans,Models; Psychological,Perception,Psychological Theory,Speech,Verbal Behavior},
  pages = {113-157},
  file = {/Users/Ladislas/Zotero/storage/N5BBALLG/Hartsuiker and Kolk - 2001 - Error monitoring in speech production a computati.pdf},
  pmid = {11259106},
}
@InCollection{mackay_constraints_1992,
  title = {Constraints on Theories of Inner Speech},
  booktitle = {Auditory Imagery},
  publisher = {{Erlbaum; Hillsdale, N. J.}},
  author = {Donald G. MacKay},
  editor = {Daniel Reisberg},
  year = {1992},
  pages = {121-149},
  file = {/Users/Ladislas/Zotero/storage/9NL9ABMQ/MacKay - 1992.pdf},
}
@Article{hurlburt_exploring_2016,
  title = {Exploring the {{Ecological Validity}} of {{Thinking}} on {{Demand}}: {{Neural Correlates}} of {{Elicited}} vs. {{Spontaneously Occurring Inner Speech}}},
  volume = {11},
  issn = {1932-6203},
  shorttitle = {Exploring the {{Ecological Validity}} of {{Thinking}} on {{Demand}}},
  abstract = {Psychology and cognitive neuroscience often use standardized tasks to elicit particular experiences. We explore whether elicited experiences are similar to spontaneous experiences. In an MRI scanner, five participants performed tasks designed to elicit inner speech (covertly repeating experimenter-supplied words), inner seeing, inner hearing, feeling, and sensing. Then, in their natural environments, participants were trained in four days of random-beep-triggered Descriptive Experience Sampling (DES). They subsequently returned to the scanner for nine 25-min resting-state sessions; during each they received four DES beeps and described those moments (9 \texttimes{} 4 = 36 moments per participant) of spontaneously occurring experience. Enough of those moments included spontaneous inner speech to allow us to compare brain activation during spontaneous inner speech with what we had found in task-elicited inner speech. ROI analysis was used to compare activation in two relevant areas (Heschl's gyrus and left inferior frontal gyrus). Task-elicited inner speech was associated with decreased activation in Heschl's gyrus and increased activation in left inferior frontal gyrus. However, spontaneous inner speech had the opposite effect in Heschl's gyrus and no significant effect in left inferior frontal gyrus. This study demonstrates how spontaneous phenomena can be investigated in MRI and calls into question the assumption that task-created phenomena are often neurophysiologically and psychologically similar to spontaneously occurring phenomena.},
  language = {en},
  number = {2},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0147932},
  author = {Russell T. Hurlburt and Ben Alderson-Day and Simone K{\"u}hn and Charles Fernyhough},
  editor = {Frederic Dick},
  month = {feb},
  year = {2016},
  pages = {e0147932},
  file = {/Users/Ladislas/Zotero/storage/CE4L3KVB/Hurlburt et al. - 2016 - Exploring the Ecological Validity of Thinking on D.pdf;/Users/Ladislas/Zotero/storage/XW4LKK9T/Hurlburt et al. - 2016 - Exploring the Ecological Validity of Thinking on D.pdf},
}
@InCollection{loevenbruck_cognitive_2018,
  title = {A {{Cognitive Neuroscience View}} of {{Inner Language}}: To Predict and to Hear, See, Feel},
  language = {en},
  booktitle = {Inner Speech: {{New}} Voices},
  publisher = {{Oxford University Press}},
  author = {H{\a'e}l{\a`e}ne L{\oe}venbruck and R Grandchamp and Lucile Rapin and Ladislas Nalborczyk and M Dohen and P Perrier and M Baciu and M Perrone-Bertolotti},
  editor = {Peter Langland-Hassan and Agust\'in Vicente},
  year = {2018},
  pages = {37},
  file = {/Users/Ladislas/Zotero/storage/B887UFJJ/LÅ“venbruck et al. - A Cognitive Neuroscience View of Inner Language.pdf},
}
@InCollection{dell_errors_1992,
  address = {{Boston, MA}},
  title = {Errors in {{Inner Speech}}},
  isbn = {978-1-4899-1164-3},
  abstract = {Many people have the feeling that they can hear a little voice inside their heads. This inner speech accompanies reading and writing and often co-occurs with activities that involve mental planning such as problem solving (Sokolov, 1972). Clearly, inner speech is ubiquitous in our mental lives, and so it is not surprising that it has played a large role in psychological theory. For example, it has been proposed that inner speech is a necessary accompaniment to thought and even that inner speech is to be identified with thought (Watson, 1919). Although these radical views of the relation between inner speech and thought are held by few, if any, psychologists today, there is, nonetheless, widespread assent that the voice in the head is important.},
  booktitle = {Experimental {{Slips}} and {{Human Error}}: {{Exploring}} the {{Architecture}} of {{Volition}}},
  publisher = {{Springer US}},
  author = {Gary S. Dell and Renee J. Repka},
  editor = {Bernard J. Baars},
  year = {1992},
  pages = {237-262},
  doi = {10.1007/978-1-4899-1164-3_10},
}
@Article{oppenheim_inner_2008,
  title = {Inner Speech Slips Exhibit Lexical Bias, but Not the Phonemic Similarity Effect},
  volume = {106},
  language = {en},
  number = {1},
  journal = {Cognition},
  doi = {10.1016/j.cognition.2007.02.006},
  author = {Gary M Oppenheim and Gary S Dell},
  year = {2008},
  pages = {528-537},
  file = {/Users/Ladislas/Zotero/storage/IWSKHQ3A/Oppenheim and Dell - 2008 - Inner speech slips exhibit lexical bias, but not t.pdf;/Users/Ladislas/Zotero/storage/LYBAFP9A/Oppenheim and Dell - 2008 - Inner speech slips exhibit lexical bias, but not t.pdf},
}
@Article{oppenheim_motor_2010,
  title = {Motor Movement Matters: {{The}} Flexible Abstractness of Inner Speech},
  volume = {38},
  issn = {0090-502X, 1532-5946},
  abstract = {Inner speech is typically characterized as either the activation of abstract linguistic representations or a detailed articulatory simulation that lacks only the production of sound. We present a study of the `speech errors' that occur during the inner recitation of tongue-twister like phrases. Two forms of inner speech were tested: inner speech without articulatory movements and articulated (mouthed) inner speech. While mouthing one's inner speech could reasonably be assumed to require more articulatory planning, prominent theories assume that such planning should not affect the experience of inner speech and consequently the errors that are `heard' during its production. The errors occurring in articulated inner speech exhibited the phonemic similarity effect and lexical bias effect, two speech-error phenomena that, in overt speech, have been localized to an articulatory-feature processing level and a lexical-phonological level, respectively. In contrast, errors in unarticulated inner speech did not exhibit the phonemic similarity effect\textemdash{}just the lexical bias effect. The results are interpreted as support for a flexible abstraction account of inner speech. This conclusion has ramifications for the embodiment of language and speech and for the theories of speech production.},
  language = {en},
  number = {8},
  journal = {Memory \& Cognition},
  doi = {10.3758/MC.38.8.1147},
  author = {Gary M Oppenheim and Gary S Dell},
  year = {2010},
  pages = {1147-1160},
  file = {/Users/Ladislas/Desktop/Articles/Oppenheim, Dell - 2010 - Motor movement matters the flexible abstractness of inner speech.pdf;/Users/Ladislas/Zotero/storage/L9BX2UUG/Oppenheim and Dell - 2010 - Motor movement matters The flexible abstractness .pdf;/Users/Ladislas/Zotero/storage/QSR8BEDE/Oppenheim and Dell - 2010 - Motor movement matters The flexible abstractness .pdf},
}
@InCollection{geva_inner_2018,
  title = {Inner {{Speech}} and {{Mental Imagery}}: {{A Neuroscientific Perspective}}},
  isbn = {978-0-19-186686-9},
  abstract = {Inner speech has been investigated using neuroscientific techniques since the beginning of the twentieth century. One of the most important finding is that inner and overt speech differ in many respects, not only in the absence/presence of articulatory movements. In addition, studies implicate the involvement of various brain regions in the production and processing of inner speech, including areas involved in phonology and semantics, as well as auditory and motor processing. By looking at parallels between inner speech and other domains of imagery, studies explore two major questions: Are there common types of representations that underlie all types of mental imagery? And, is there a neural substrate for imagery, above and beyond modality? While these questions cannot yet be fully answered, studies of the neuroscience of imagery are bringing us a step towards better understanding of inner speech.},
  language = {en\_US},
  booktitle = {Inner {{Speech}}: {{New Voices}}},
  publisher = {{Oxford University Press}},
  author = {Sharon Geva},
  editor = {Peter Langland-Hassan and Agustin Vicente},
  month = {oct},
  year = {2018},
  file = {/Users/Ladislas/Zotero/storage/AK8IQLAA/oso-9780198796640-chapter-5.html},
}
@Article{tian_mental_2016,
  title = {Mental Imagery of Speech Implicates Two Mechanisms of Perceptual Reactivation},
  volume = {77},
  issn = {00109452},
  abstract = {Sensory cortices can be activated without any external stimuli. Yet, it is still unclear how this perceptual reactivation occurs and which neural structures mediate this reconstruction process. In this study, we employed fMRI with mental imagery paradigms to investigate the neural networks involved in perceptual reactivation. Subjects performed two speech imagery tasks: articulation imagery (AI) and hearing imagery (HI). We found that AI induced greater activity in frontal-parietal sensorimotor systems, including sensorimotor cortex, subcentral (BA 43), middle frontal cortex (BA 46) and parietal operculum (PO), whereas HI showed stronger activation in regions that have been implicated in memory retrieval: middle frontal (BA 8), inferior parietal cortex and intraparietal sulcus. Moreover, posterior superior temporal sulcus (pSTS) and anterior superior temporal gyrus (aSTG) was activated more in AI compared with HI, suggesting that covert motor processes induced stronger perceptual reactivation in the auditory cortices. These results suggest that motorto-perceptual transformation and memory retrieval act as two complementary mechanisms to internally reconstruct corresponding perceptual outcomes. These two mechanisms can serve as a neurocomputational foundation for predicting perceptual changes, either via a previously learned relationship between actions and their perceptual consequences or via stored perceptual experiences of stimulus and episodic or contextual regularity.},
  language = {en},
  journal = {Cortex},
  doi = {10.1016/j.cortex.2016.01.002},
  author = {Xing Tian and Jean Mary Zarate and David Poeppel},
  month = {apr},
  year = {2016},
  pages = {1-12},
  file = {/Users/Ladislas/Zotero/storage/ZFPNRK5L/Tian et al. - 2016 - Mental imagery of speech implicates two mechanisms.pdf},
}

@Article{tian_mental_2010,
  title = {Mental Imagery of Speech and Movement Implicates the Dynamics of Internal Forward Models},
  volume = {1},
  issn = {16641078},
  abstract = {The classical concept of efference copies in the context of internal forward models has stimulated productive research in cognitive science and neuroscience. There are compelling reasons to argue for such a mechanism, but finding direct evidence in the human brain remains difficult. Here we investigate the dynamics of internal forward models from an unconventional angle: mental imagery, assessed while recording high temporal resolution neuronal activity using magnetoencephalography. We compare two overt and covert tasks; our covert, mental imagery tasks are unconfounded by overt input/output demands \textendash{} but in turn necessitate the development of appropriate multi-dimensional topographic analyses. Finger tapping (studies 1 and 2) and speech experiments (studies 3\textendash{}5) provide temporally constrained results that implicate the estimation of an efference copy. We suggest that one internal forward model over parietal cortex subserves the kinesthetic feeling in motor imagery. Secondly, observed auditory neural activity {$\sim$}170 ms after motor estimation in speech experiments (studies 3\textendash{}5) demonstrates the anticipated auditory consequences of planned motor commands in a second internal forward model in imagery of speech production. Our results provide neurophysiological evidence from the human brain in favor of internal forward models deploying efference copies in somatosensory and auditory cortex, in finger tapping and speech production tasks, respectively, and also suggest the dynamics and sequential updating structure of internal forward models.},
  language = {en},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2010.00166},
  author = {Xing Tian},
  year = {2010},
  file = {/Users/Ladislas/Zotero/storage/9C5FFBIK/Tian - 2010 - Mental imagery of speech and movement implicates t.pdf;/Users/Ladislas/Zotero/storage/WMH29K3W/Tian - 2010 - Mental imagery of speech and movement implicates t.pdf},
}

@Article{tian_mental_2012,
  title = {Mental Imagery of Speech: Linking Motor and Perceptual Systems through Internal Simulation and Estimation},
  volume = {6},
  issn = {1662-5161},
  shorttitle = {Mental Imagery of Speech},
  abstract = {The neural basis of mental imagery has been investigated by localizing the underlying neural networks, mostly in motor and perceptual systems, separately. However, how modality-specific representations are top-down induced and how the action and perception systems interact in the context of mental imagery is not well understood. Imagined speech production (`articulation imagery'), which induces the kinesthetic feeling of articulator movement and its auditory consequences, provides a new angle because of the concurrent involvement of motor and perceptual systems. On the basis of previous findings in mental imagery of speech, we argue for the following regarding the induction mechanisms of mental imagery and the interaction between motor and perceptual systems: (1) Two distinct top-down mechanisms, memory retrieval and motor simulation, exist to induce estimation in perceptual systems. (2) Motor simulation is sufficient to internally induce the representation of perceptual changes that would be caused by actual movement (perceptual associations); however, this simulation process only has modulatory effects on the perception of external stimuli, which critically depends on context and task demands. Considering the proposed simulation-estimation processes as common mechanisms for interaction between motor and perceptual systems, we outline how mental imagery (of speech) relates to perception and production, and how these hypothesized mechanisms might underpin certain neural disorders.},
  language = {English},
  journal = {Frontiers in Human Neuroscience},
  doi = {10.3389/fnhum.2012.00314},
  author = {Xing Tian and David Poeppel},
  year = {2012},
  keywords = {auditory hallucination,corollary discharge,efference copy,internal forward model,Mirror Neurons,Phantom Limb,sensory-motor integration,Stuttering},
  file = {/Users/Ladislas/Zotero/storage/YYJQF37D/Tian and Poeppel - 2012 - Mental imagery of speech linking motor and percep.pdf},
}
@InProceedings{kapur_alterego_2018,
  address = {{Tokyo, Japan}},
  title = {{{AlterEgo}}: {{A Personalized Wearable Silent Speech Interface}}},
  isbn = {978-1-4503-4945-1},
  shorttitle = {{{AlterEgo}}},
  abstract = {We present a wearable interface that allows a user to silently converse with a computing device without any voice or any discernible movements - thereby enabling the user to communicate with devices, AI assistants, applications or other people in a silent, concealed and seamless manner. A user's intention to speak and internal speech is characterized by neuromuscular signals in internal speech articulators that are captured by the AlterEgo system to reconstruct this speech. We use this to facilitate a natural language user interface, where users can silently communicate in natural language and receive aural output (e.g - bone conduction headphones), thereby enabling a discreet, bi-directional interface with a computing device, and providing a seamless form of intelligence augmentation. The paper describes the architecture, design, implementation and operation of the entire system. We demonstrate robustness of the system through user studies and report 92\% median word accuracy levels.},
  language = {en},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Human Information Interaction}}\&{{Retrieval}}  - {{IUI}} '18},
  publisher = {{ACM Press}},
  doi = {10.1145/3172944.3172977},
  author = {Arnav Kapur and Shreyas Kapur and Pattie Maes},
  year = {2018},
  pages = {43-53},
  file = {/Users/Ladislas/Zotero/storage/K4XUCBCD/Kapur et al. - 2018 - AlterEgo A Personalized Wearable Silent Speech In.pdf},
}

@Article{martin_decoding_2014,
  title = {Decoding Spectrotemporal Features of Overt and Covert Speech from the Human Cortex},
  volume = {7},
  issn = {1662-6443},
  abstract = {Auditory perception and auditory imagery have been shown to activate overlapping brain regions. We hypothesized that these phenomena also share a common underlying neural representation. To assess this, we used electrocorticography intracranial recordings from epileptic patients performing an out loud or a silent reading task. In these tasks, short stories scrolled across a video screen in two conditions: subjects read the same stories both aloud (overt) and silently (covert). In a control condition the subject remained in a resting state. We first built a high gamma (70\textendash{}150 Hz) neural decoding model to reconstruct spectrotemporal auditory features of self-generated overt speech. We then evaluated whether this same model could reconstruct auditory speech features in the covert speech condition. Two speech models were tested: a spectrogram and a modulation-based feature space. For the overt condition, reconstruction accuracy was evaluated as the correlation between original and predicted speech features, and was significant in each subject (p {$<$} 10-5; paired two-sample t-test). For the covert speech condition, dynamic time warping was first used to realign the covert speech reconstruction with the corresponding original speech from the overt condition. Reconstruction accuracy was then evaluated as the correlation between original and reconstructed speech features. Covert reconstruction accuracy was compared to the accuracy obtained from reconstructions in the baseline control condition. Reconstruction accuracy for the covert condition was significantly better than for the control condition (p {$<$} 0.005; paired two-sample t-test). The superior temporal gyrus, pre- and post-central gyrus provided the highest reconstruction information. The relationship between overt and covert speech reconstruction depended on anatomy. These results provide evidence that auditory representations of covert speech can be reconstructed from models that are built from an overt speech data set, supporting a partially shared neural substrate.},
  language = {en},
  journal = {Frontiers in Neuroengineering},
  doi = {10.3389/fneng.2014.00014},
  author = {St{\~A}{\textcopyright}phanie Martin and Peter Brunner and Chris Holdgraf and Hans-Jochen Heinze and Nathan E. Crone and Jochem Rieger and Gerwin Schalk and Robert T. Knight and Brian N. Pasley},
  month = {may},
  year = {2014},
  file = {/Users/Ladislas/Zotero/storage/GMZWJVQJ/Martin et al. - 2014 - Decoding spectrotemporal features of overt and cov.pdf},
}

@Article{martin_word_2016,
  title = {Word Pair Classification during Imagined Speech Using Direct Brain Recordings},
  volume = {6},
  issn = {2045-2322},
  language = {en},
  number = {1},
  journal = {Scientific Reports},
  doi = {10.1038/srep25803},
  author = {Stephanie Martin and Peter Brunner and I{\~n}aki Iturrate and Jos{\a'e} del R. Mill{\a'a}n and Gerwin Schalk and Robert T. Knight and Brian N. Pasley},
  month = {sep},
  year = {2016},
  file = {/Users/Ladislas/Zotero/storage/34J3ZG5H/Martin et al. - 2016 - Word pair classification during imagined speech us.pdf},
}

@InProceedings{kapur_alterego_2018,
  address = {{Tokyo, Japan}},
  title = {{{AlterEgo}}: {{A Personalized Wearable Silent Speech Interface}}},
  isbn = {978-1-4503-4945-1},
  shorttitle = {{{AlterEgo}}},
  abstract = {We present a wearable interface that allows a user to silently converse with a computing device without any voice or any discernible movements - thereby enabling the user to communicate with devices, AI assistants, applications or other people in a silent, concealed and seamless manner. A user's intention to speak and internal speech is characterized by neuromuscular signals in internal speech articulators that are captured by the AlterEgo system to reconstruct this speech. We use this to facilitate a natural language user interface, where users can silently communicate in natural language and receive aural output (e.g - bone conduction headphones), thereby enabling a discreet, bi-directional interface with a computing device, and providing a seamless form of intelligence augmentation. The paper describes the architecture, design, implementation and operation of the entire system. We demonstrate robustness of the system through user studies and report 92\% median word accuracy levels.},
  language = {en},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Human Information Interaction}}\&{{Retrieval}}  - {{IUI}} '18},
  publisher = {{ACM Press}},
  doi = {10.1145/3172944.3172977},
  author = {Arnav Kapur and Shreyas Kapur and Pattie Maes},
  year = {2018},
  pages = {43-53},
  file = {/Users/Ladislas/Zotero/storage/QF6PF57K/Kapur et al. - 2018 - AlterEgo A Personalized Wearable Silent Speech In.pdf},
}
@Article{glover_motor-cognitive_2017,
  title = {The Motor-Cognitive Model of Motor Imagery: {{Evidence}} from Timing Errors in Simulated Reaching and Grasping.},
  volume = {43},
  issn = {1939-1277, 0096-1523},
  shorttitle = {The Motor-Cognitive Model of Motor Imagery},
  abstract = {Understanding motor imagery has important implications for its use in therapeutic interventions, as well as in understanding the human mind. The present article presents the motor-cognitive model of motor imagery. Three experiments were reported that found that motor imagery can be more affected by task precision and interference from secondary tasks than overt actions, consistent with this theory but not with the currently popular functional equivalence model.},
  language = {en},
  number = {7},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  doi = {10.1037/xhp0000389},
  author = {Scott Glover and Marek Baran},
  year = {2017},
  pages = {1359-1375},
  file = {/Users/Ladislas/Zotero/storage/3T68IZYB/Glover and Baran - 2017 - The motor-cognitive model of motor imagery Eviden.pdf},
}
@InCollection{wilkinson_auditory_2017,
  address = {{Exeter (UK)}},
  series = {Wellcome {{Trust}}\textendash{{Funded Monographs}} and {{Book Chapters}}},
  title = {Auditory {{Verbal Hallucinations}} and {{Inner Speech}}: {{A Predictive Processing Perspective}}},
  copyright = {Copyright \textcopyright{} Imprint Academic, 2017. Individual contributions \textcopyright{} the respective authors 2017.},
  isbn = {978-1-84540-920-3},
  lccn = {NBK447654},
  shorttitle = {Auditory {{Verbal Hallucinations}} and {{Inner Speech}}},
  abstract = {Inner speech is a pervasive feature of our conscious lives. But what is inner speech, and what happens in unconscious processing that makes it the conscious experience that it is? A clue to answering this can be found in cases where the mechanisms that produce inner speaking behave unusually. In this paper, we suggest an account of a specific instance of this, namely, a particular subtype of auditory verbal hallucination (AVH), and draw some lessons about the processes that underlie normal inner speech.},
  language = {eng},
  booktitle = {Before {{Consciousness}}: {{In Search}} of the {{Fundamentals}} of {{Mind}}},
  publisher = {{Imprint Academic, Ltd.}},
  author = {Sam Wilkinson and Charles Fernyhough},
  editor = {Zdravko Radman},
  year = {2017},
  file = {/Users/Ladislas/Zotero/storage/6MU682EY/NBK447654.html},
  pmid = {28825786},
}
@Article{ito_control_2008,
  title = {Control of Mental Activities by Internal Models in the Cerebellum},
  volume = {9},
  issn = {1471-003X, 1471-0048},
  abstract = {The intricate neuronal circuitry of the cerebellum is thought to encode internal models that reproduce the dynamic properties of body parts. These models are essential for controlling the movement of these body parts: they allow the brain to precisely control the movement without the need for sensory feedback. It is thought that the cerebellum might also encode internal models that reproduce the essential properties of mental representations in the cerebral cortex. This hypothesis suggests a possible mechanism by which intuition and implicit thought might function and explains some of the symptoms that are exhibited by psychiatric patients. This article examines the conceptual bases and experimental evidence for this hypothesis.},
  language = {en},
  number = {4},
  journal = {Nature Reviews Neuroscience},
  doi = {10.1038/nrn2332},
  author = {Masao Ito},
  month = {apr},
  year = {2008},
  pages = {304-313},
  file = {/Users/Ladislas/Zotero/storage/BVXVBQJD/Ito - 2008 - Control of mental activities by internal models in.pdf;/Users/Ladislas/Zotero/storage/TM3V7T5Z/Ito - 2008 - Control of mental activities by internal models in.pdf},
}
@Article{whitford_neurophysiological_2017,
  title = {Neurophysiological Evidence of Efference Copies to Inner Speech},
  volume = {6},
  issn = {2050-084X},
  language = {en},
  journal = {eLife},
  doi = {10.7554/eLife.28197},
  author = {Thomas J Whitford and Bradley N Jack and Daniel Pearson and Oren Griffiths and David Luque and Anthony WF Harris and Kevin M Spencer and Mike E {Le Pelley}},
  month = {dec},
  year = {2017},
  file = {/Users/Ladislas/Zotero/storage/WIRD8M3E/Whitford et al. - 2017 - Neurophysiological evidence of efference copies to.pdf},
}

@Article{Jones2007,
  title = {Thought as Action: Inner Speech, Self-Monitoring, and Auditory Verbal Hallucinations.},
  volume = {16},
  issn = {1053-8100},
  abstract = {Passivity experiences in schizophrenia are thought to be due to a failure in a neurocognitive action self-monitoring system (NASS). Drawing on the assumption that inner speech is a form of action, a recent model of auditory verbal hallucinations (AVHs) has proposed that AVHs can be explained by a failure in the NASS. In this article, we offer an alternative application of the NASS to AVHs, with separate mechanisms creating the emotion of self-as-agent and other-as-agent. We defend the assumption that inner speech can be considered as a form of action, and show how a number of previous criticisms of applying the NASS to AVHs can be refuted. This is achieved in part through taking a Vygotskian developmental perspective on inner speech. It is suggested that more research into the nature and development of inner speech is needed to further our understanding of AVHs.},
  number = {2},
  journal = {Consciousness and cognition},
  doi = {10.1016/j.concog.2005.12.003},
  author = {Simon R Jones and Charles Fernyhough},
  month = {jun},
  year = {2007},
  keywords = {Humans,Models; Psychological,Hallucinations,Hallucinations: psychology,Ego,Schizophrenic Psychology},
  pages = {391-9},
  file = {/Users/Ladislas/Desktop/Articles/Jones, Fernyhough - 2007 - Thought as action inner speech, self-monitoring, and auditory verbal hallucinations.pdf;/Users/Ladislas/Zotero/storage/77RSXELY/Jones and Fernyhough - 2007 - Thought as action Inner speech, self-monitoring, .pdf;/Users/Ladislas/Zotero/storage/TQQX547A/Jones and Fernyhough - 2007 - Thought as action Inner speech, self-monitoring, .pdf;/Users/Ladislas/Zotero/storage/X3HJF7GW/Jones and Fernyhough - 2007 - Thought as action Inner speech, self-monitoring, .pdf},
  pmid = {16464616},
}

@Article{tian_imagined_2018,
  title = {Imagined Speech Influences Perceived Loudness of Sound},
  volume = {2},
  issn = {2397-3374},
  language = {en},
  number = {3},
  journal = {Nature Human Behaviour},
  doi = {10.1038/s41562-018-0305-8},
  author = {Xing Tian and Nai Ding and Xiangbin Teng and Fan Bai and David Poeppel},
  month = {mar},
  year = {2018},
  pages = {225-234},
  file = {/Users/Ladislas/Zotero/storage/Y994KZ4E/Tian et al. - 2018 - Imagined speech influences perceived loudness of s.pdf},
}

@Article{jones_neural_2007,
  title = {Neural Correlates of Inner Speech and Auditory Verbal Hallucinations: {{A}} Critical Review and Theoretical Integration},
  volume = {27},
  issn = {0272-7358},
  shorttitle = {Neural Correlates of Inner Speech and Auditory Verbal Hallucinations},
  abstract = {The neuroimaging and neurophysiological literature on inner speech in healthy participants and those who experience auditory verbal hallucinations (AVHs) is reviewed. AVH-hearers in remission and controls do not differ neurologically on tasks involving low levels of verbal self-monitoring (VSM), such as reciting sentences in inner speech. In contrast, on tasks involving high levels of VSM, such as auditory verbal imagery, AVH-hearers in remission show less activation in areas including the middle and superior temporal gyri. This pattern of findings leads to a conundrum, given that mentation involving low levels of VSM is typically held to form the raw material for AVHs. We address this by noting that existing neuroimaging and neurophysiological studies have been based on unexamined assumptions about the form and developmental significance of inner speech. We set out a Vygotskian approach to AVHs which can account for why they are generally experienced as the voice of another person, with specific acoustic properties, and a tendency to take the form of commands. On this approach, which we argue is consistent with the neural correlates evidence, AVHs result from abnormalities in the transition between condensed and expanded dialogic inner speech. Further potential empirical tests of this model are discussed.},
  number = {2},
  journal = {Clinical Psychology Review},
  doi = {10.1016/j.cpr.2006.10.001},
  author = {Simon R. Jones and Charles Fernyhough},
  month = {mar},
  year = {2007},
  keywords = {Inner speech,Auditory verbal hallucinations,Dialogue,Verbal self-monitoring,Vygotsky},
  pages = {140-154},
  file = {/Users/Ladislas/Zotero/storage/R3BRUC3H/S0272735806001152.html},
}

@Article{ford_electrophysiological_2004,
  title = {Electrophysiological Evidence of Corollary Discharge Dysfunction in Schizophrenia during Talking and Thinking},
  volume = {38},
  issn = {0022-3956},
  abstract = {Failure of corollary discharge, a mechanism for distinguishing self-generated from externally-generated percepts, has been posited to underlie certain positive symptoms of schizophrenia, including auditory hallucinations. Although originally described in the visual system, corollary discharge may exist in the auditory system, whereby signals from motor speech commands prepare auditory cortex for self-generated speech. While associated with sensorimotor systems, it might also apply to inner speech or thought, regarded as our most complex motor act. We had four aims in the studies summarized in this paper: (1) to demonstrate the corollary discharge phenomenon during talking and inner speech in human volunteers using event-related brain potentials (ERPs), (2) to demonstrate that the corollary discharge is abnormal in patients with schizophrenia, (3) to demonstrate the role of frontal speech areas in the corollary discharge during talking, and (4) to relate the dysfunction of the corollary discharge in schizophrenia to auditory hallucinations. Using EEG and ERP measures, we addressed each aim in patients with schizophrenia (DSM IV) and healthy control subjects. The N1 component of the ERP reflected dampening of auditory cortex responsivity during talking and inner speech in control subjects but not in patients. EEG measures of coherence indicated inter-dependence of activity in the frontal speech production and temporal speech reception areas during talking in control subjects, but not in patients, especially those who hallucinated. These data suggest that a corollary discharge from frontal areas where thoughts are generated fails to alert auditory cortex that they are self-generated, leading to the misattribution of inner speech to external sources and producing the experience of auditory hallucinations.},
  number = {1},
  journal = {Journal of Psychiatric Research},
  doi = {10.1016/S0022-3956(03)00095-5},
  author = {Judith M. Ford and Daniel H. Mathalon},
  month = {jan},
  year = {2004},
  keywords = {Corollary discharge,EEG coherence,N1,Schizophrenia},
  pages = {37-46},
  file = {/Users/Ladislas/Zotero/storage/78M23V53/S0022395603000955.html},
}
@Article{feinberg_efference_1978,
  title = {Efference {{Copy}} and {{Corollary Discharge}}: {{Implications}} for {{Thinking}} and {{Its Disorders}}},
  volume = {4},
  issn = {0586-7614, 1745-1701},
  shorttitle = {Efference {{Copy}} and {{Corollary Discharge}}},
  language = {en},
  number = {4},
  journal = {Schizophrenia Bulletin},
  doi = {10.1093/schbul/4.4.636},
  author = {I. Feinberg},
  month = {jan},
  year = {1978},
  pages = {636-640},
  file = {/Users/Ladislas/Zotero/storage/RQQXLAJP/Feinberg - 1978 - Efference Copy and Corollary Discharge Implicatio.pdf},
}

@Article{frith_role_1996,
  title = {The Role of the Prefrontal Cortex in Self-Consciousness: The Case of Auditory Hallucinations},
  volume = {351},
  issn = {0962-8436},
  shorttitle = {The Role of the Prefrontal Cortex in Self-Consciousness},
  abstract = {Many patients with schizophrenia report hallucinations in which they hear voices talking to them or about them. Behavioural and physiological studies show that this experience is associated with processes occurring in auditory language systems associated with both the production and the reception of speech. I propose that hallucinations are experienced because patients have difficulty in distinguishing sensations caused by their own actions from those that arise from external influences. This distinction can be made by predicting the sensations that will result from executive commands (forward modelling). If the predicted sensation matches the actual sensation then no outside influences have occurred and perception of change can be 'cancelled'. At the physiological level this mechanism depends upon interactions between the prefrontal areas where the executive commands originate and posterior brain regions concerned with the resultant sensations. Evidence from functional brain imaging confirms that interactions between prefrontal (executive) areas and auditory association areas are abnormal in schizophrenia. However, this account needs to be extended before we can understand why patients experience the voices as emanating, not just from an external source, but from agents who are trying to influence their behaviour. Recent imaging studies suggest that medial prefrontal cortex is engaged when we think about other people, but the precise nature of the interaction of this brain area with other regions remains to be established.},
  language = {eng},
  number = {1346},
  journal = {Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences},
  doi = {10.1098/rstb.1996.0136},
  author = {C. Frith},
  month = {oct},
  year = {1996},
  keywords = {Consciousness,Hallucinations,Hearing,Humans,Memory,Models; Biological,Neural Pathways,Neuropsychological Tests,Prefrontal Cortex,Schizophrenia},
  pages = {1505-1512},
  pmid = {8941962},
}
@Book{jeannerod_motor_2006,
  address = {{Oxford ; New York}},
  series = {Oxford Psychology Series},
  title = {Motor Cognition: What Actions Tell the Self},
  isbn = {978-0-19-856964-0 978-0-19-856965-7},
  lccn = {QP376 .J415 2006},
  shorttitle = {Motor Cognition},
  number = {no. 42},
  publisher = {{Oxford University Press}},
  author = {Marc Jeannerod},
  year = {2006},
  keywords = {Brain,Cognition,Mental Processes,Motor ability,Motor Activity,physiology,Physiology,Psychomotor Performance},
  file = {/Users/Ladislas/Zotero/storage/B26GAPYP/Jeannerod - 2006 - Motor cognition what actions tell the self.pdf;/Users/Ladislas/Zotero/storage/BXDRK82G/Jeannerod - 2006.pdf},
  note = {OCLC: ocm65195268},
}
@Article{guillot_imagining_2012,
  title = {Imagining Is {{Not Doing}} but {{Involves Specific Motor Commands}}: {{A Review}} of {{Experimental Data Related}} to {{Motor Inhibition}}},
  volume = {6},
  issn = {1662-5161},
  shorttitle = {Imagining Is {{Not Doing}} but {{Involves Specific Motor Commands}}},
  journal = {Frontiers in Human Neuroscience},
  doi = {10.3389/fnhum.2012.00247},
  author = {Aymeric Guillot and Franck {Di Rienzo} and Tadhg MacIntyre and Aidan Moran and Christian Collet},
  year = {2012},
  file = {/Users/Ladislas/Zotero/storage/GU2AEFBG/Guillot et al. - 2012 - Imagining is Not Doing but Involves Specific Motor.pdf;/Users/Ladislas/Zotero/storage/PWLYRYJV/Guillot et al. 2012 Front Hum Neurosci.pdf;/Users/Ladislas/Zotero/storage/RSSFUB2N/Guillot et al. - 2012 - Imagining is Not Doing but Involves Specific Motor.pdf},
}
@Article{guillot_understanding_2012,
  title = {Understanding the Timing of Motor Imagery: Recent Findings and Future Directions},
  volume = {5},
  issn = {1750-984X, 1750-9858},
  shorttitle = {Understanding the Timing of Motor Imagery},
  language = {en},
  number = {1},
  journal = {International Review of Sport and Exercise Psychology},
  doi = {10.1080/1750984X.2011.623787},
  author = {Aymeric Guillot and Nady Hoyek and Magali Louis and Christian Collet},
  month = {mar},
  year = {2012},
  pages = {3-22},
  file = {/Users/Ladislas/Zotero/storage/52H9W5ZY/Guillot et al. - 2012 - Understanding the timing of motor imagery recent .pdf},
}
@InCollection{collet_autonomic_2010,
  title = {Autonomic Nervous System Activities during Imagined Movements},
  abstract = {This chapter explores the experimental studies investigating motor imagery through the recording of autonomic nervous system activity. It outlines the goals and methods of such peripheral recordings in studying mental processes. It also discusses how the motor commands sent to the autonomic effectors are facilitated during motor imagery, whereas the direct voluntary commands transmitted through the pyramidal tract are at least partially inhibited.},
  language = {en},
  booktitle = {The Neurophysiological Foundations of Mental and Motor Imagery},
  publisher = {{Oxford University Press}},
  author = {Christian Collet and Aymeric Guillot},
  editor = {Aymeric Guillot and Christian Collet},
  year = {2010},
  file = {/Users/Ladislas/Zotero/storage/7BVY7TCD/Guillot and Collet - 2017 - Oxford Scholarship Online.pdf},
}
@Article{rieger_inhibition_2017,
  title = {Inhibition in Motor Imagery: A Novel Action Mode Switching Paradigm},
  volume = {24},
  issn = {1069-9384, 1531-5320},
  shorttitle = {Inhibition in Motor Imagery},
  language = {en},
  number = {2},
  journal = {Psychonomic Bulletin \& Review},
  doi = {10.3758/s13423-016-1095-5},
  author = {Martina Rieger and Stephan F. Dahm and Iring Koch},
  month = {apr},
  year = {2017},
  pages = {459-466},
  file = {/Users/Ladislas/Zotero/storage/AFAGJXFA/Rieger, Dahm, & Koch - 2017.pdf},
}

@Article{dahm_cognitive_2016,
  title = {Cognitive Constraints on Motor Imagery},
  volume = {80},
  issn = {0340-0727, 1430-2772},
  language = {en},
  number = {2},
  journal = {Psychological Research},
  doi = {10.1007/s00426-015-0656-y},
  author = {Stephan F. Dahm and Martina Rieger},
  month = {mar},
  year = {2016},
  pages = {235-247},
  file = {/Users/Ladislas/Zotero/storage/X9QHWFT8/Dahm & Rieger - 2016.pdf},
}
@InCollection{baddeley_working_1974,
  title = {Working {{Memory}}},
  volume = {8},
  abstract = {This chapter presents a body of new experimental evidence, which provides a firm basis for the working memory hypothesis. The chapter presents a series of experiments on the role of memory in reasoning, language comprehension, and learning. An attempt is made to apply the comparable techniques in all three cases to allow a common pattern to emerge, if the same working memory system is operative in all three instances. The chapter makes a case for postulating the working memory-LTS system as a modification of the current STS-LTS view. Working memory represents a control system with limits on both its storage and processing capabilities, and has access to phonemically coded information (possibly by controlling a rehearsal buffer), that it is responsible for the limited memory span, but does not underlie the recency effect in free recall. The experiments presented in the chapter suggest that the phonemic rehearsal buffer plays a limited role in this process, but is by no means essential. These experiments also suggest that working memory plays a part in verbal reasoning and in prose comprehension. Understanding the detailed role of working memory in these tasks, however, must proceed hand-in-hand with an understanding of the tasks themselves.},
  booktitle = {Psychology of {{Learning}} and {{Motivation}}},
  publisher = {{Academic Press}},
  author = {Alan D. Baddeley and Graham Hitch},
  editor = {Gordon H. Bower},
  month = {jan},
  year = {1974},
  pages = {47-89},
  file = {/Users/Ladislas/Zotero/storage/S7AXVGJU/S0079742108604521.html},
  doi = {10.1016/S0079-7421(08)60452-1},
}
@Article{baddeley_episodic_2000,
  title = {The Episodic Buffer: A New Component of Working Memory?},
  volume = {4},
  issn = {1364-6613, 1879-307X},
  shorttitle = {The Episodic Buffer},
  language = {English},
  number = {11},
  journal = {Trends in Cognitive Sciences},
  doi = {10.1016/S1364-6613(00)01538-2},
  author = {Alan Baddeley},
  month = {nov},
  year = {2000},
  pages = {417-423},
  file = {/Users/Ladislas/Zotero/storage/VDMN5EV7/Baddeley - 2000 - The episodic buffer a new component of working me.pdf;/Users/Ladislas/Zotero/storage/CN68CCLD/S1364-6613(00)01538-2.html},
  pmid = {11058819},
}
@Article{martin_decoding_2018,
  title = {Decoding {{Inner Speech Using Electrocorticography}}: {{Progress}} and {{Challenges Toward}} a {{Speech Prosthesis}}},
  volume = {12},
  issn = {1662-453X},
  shorttitle = {Decoding {{Inner Speech Using Electrocorticography}}},
  abstract = {Certain brain disorders resulting from brainstem infarcts, traumatic brain injury, cerebral palsy, stroke and amyotrophic lateral sclerosis, limit verbal communication despite the patient being fully aware. People that cannot communicate due to neurological disorders would benefit from a system that can infer internal speech directly from brain signals. In this review article, we describe the state of the art in decoding inner speech, ranging from early acoustic sound features, to higher order speech units. We focused on intracranial recordings, as this technique allows monitoring brain activity with high spatial, temporal, and spectral resolution, and therefore is a good candidate to investigate inner speech. Despite intense efforts, investigating how the human cortex encodes inner speech remains an elusive challenge, due to the lack of behavioral and observable measures. We emphasize various challenges commonly encountered when investigating inner speech, and propose potential solutions in order to get closer to a natural speech assistive device.},
  language = {English},
  journal = {Frontiers in Neuroscience},
  doi = {10.3389/fnins.2018.00422},
  author = {Stephanie Martin and I{\~n}aki Iturrate and Jos{\a'e} del R. Mill{\a'a}n and Robert T. Knight and Brian N. Pasley},
  year = {2018},
  keywords = {inner speech,Brain-computer interface,Decoding,electrocorticography,neuroprosthetics},
  file = {/Users/Ladislas/Zotero/storage/U39N43KB/Martin et al. - 2018 - Decoding Inner Speech Using Electrocorticography .pdf},
}
