# Articulatory suppression effects on induced rumination

Summary of the research...^[This experimental chapter is a manuscript reformatted for the need of this thesis. Source: The manuscript has been submitted to Psychological Research. Pre-registered protocol, preprint, data, as well as reproducible code and figures are available at: https://osf.io/3bh67/.]

```{r setup, echo = FALSE, message = FALSE, warning = FALSE, results = "hide"}
library(AICcmodavg)
library(DiagrammeR)
library(tidyverse)
library(effsize)
library(ggforce)
library(plotly)
library(papaja)
library(GGally)
library(broom)
library(Rmisc)
library(MuMIn)
library(knitr)
library(lme4)

knitr::opts_chunk$set(cache = TRUE, echo = FALSE)
```

## Introduction

A large part of our inner experience involves verbal content, with internal monologues and conversations. Inner speech is considered as a major component of conscious experience and cognition [@Hubbard2010;@Klinger1987;@Hurlburt2013]. An important issue related to inner speech concerns its format and nature and whether it is better described as a mere evocation of abstract amodal verbal representations or as a concrete motor simulation of actual speech production. In the first case, inner speech is seen as divorced from bodily experience, and includes, at most, faded auditory representations. In the second case, inner speech is considered as a physical process that unfolds over time, leading to an enactive re-creation of auditory a well as articulatory percepts. The latter hypothesis is interesting in the context of persistent negative and maladaptive forms of inner speech, such as rumination. If this hypothesis is correct, we could expect rumination --as a particular type of inner speech-- to be disrupted by concurrent involvement of the speech muscles.

### Multisensory and motor components of inner speech

Introspective explorations of the characteristics of inner speech have led to different views on the relative importance of its auditory and articulatory components, and on the involvement of motor processes. For @Stricker1880, "speech representations are motor representations", while @Egger1881 believed that auditory representations are dominant in inner speech and that motor representations are not always present. However, as noted by @Ballet1886, these contradictory hypotheses might stem from an over-generalization of their own introspective findings, the former discovering motor feelings and the latter auditory images [@Ballet1886]. In the same vein, @Paulhan1886 claimed that inner speech involves both auditory and motor images, defining motor images as the sensations in the speech organs (larynx, tongue, lips) that sometimes accompany inner speech. Therefore, Paulhan’s notion of *motor images* are in fact related to somatosensory representations rather than to the involvement of actual speech movements (as claimed by Stricker). It remains, however, that a distinction was introduced by 19th century authors between sensory and motor phenomena in inner speech, with sensory phenomena including auditory as well as articulatory (or somatosensory) percepts. The intuitive distinction between auditory and motor phenomena is referred to in contemporary research by the terms of *inner ear* and *inner voice*, in line with Baddeley's classic model of working memory [e.g., @Baddeley1974; see also @Buchsbaum2013]. Baddeley's model relies on a partnership between an *inner ear* (i.e., storage) and an *inner voice* [i.e., subvocal rehearsal; see @Smith1995].

Empirical arguments supporting the crucial role of the inner voice in verbal memory (subvocal rehearsal) and auditory imagery can be found in studies using articulatory suppression, in which the *action* component (i.e., the *inner voice*) of inner speech is disrupted. Articulatory suppression usually refers to a task which requires participants to utter speech sounds (or to produce speech gestures without sound), so that this activity disrupts ongoing speech production processes. Articulatory suppression can be produced with different degrees of vocalisation, going from overt uttering of irrelevant words, to whispering, mouthing (i.e., silent articulation), and simple clamping of the speech articulators. Many verbal working memory studies have shown that articulatory suppression impairs recall performance [e.g.,@Baddeley1984].

In a study aiming at investigating the role of *covert enactment* in auditory imagery, @Reisberg1989 observed that the verbal transformation effect [VTE, @Warren1958], namely the alteration of speech percepts when certain speech sounds are uttered in a repetitive way, also occurred during inner speech (although the VTE was smaller than during overt speech), but was suppressed by concurrent articulation (e.g., chewing) or clamping the articulators. The fact that the VTE was observed during inner speech and that it was reduced by concurrent chewing, even in inner speech, speaks in favour of the view of inner speech as an enacted simulation of overt speech.

Another piece of evidence for the effect of articulatory suppression on inner speech comes from a recent study by @Topolinski2009 on the mere exposure effect, namely the fact that repeated exposure to a stimulus influences the evaluation of this stimulus in a positive way [@Zajonc1968]. Topolinski and Strack’s study showed that the mere exposure effect for visually presented verbal material could be completely suppressed by blocking subvocal rehearsal (i.e., inner speech) when asking participants to chew a gum. The effect was preserved, however, when participants kneaded a soft ball with their hand [@Topolinski2009]. This finding suggests that blocking speech motor simulation interfered with the inner rehearsal of the visually presented verbal stimuli, thereby destroying the positive exposure effect. It provides additional experimental support to the view that inner speech involves a motor component.

The occurrence of motor simulation during inner speech is further backed by several studies using physiological measures to evaluate inner speech production properties. Using electrodes inserted in the tongue tip or lips of five participants, @Jacobson1931 was able to detect electromyographic (EMG) activity during several tasks requiring inner speech. Similarly, @Sokolov1972 recorded intense lip and tongue muscle activation when participants had to perform complex tasks that necessitated substantial inner speech production (e.g., problem solving). Another study using surface electromyography (sEMG) demonstrated an increase in activity of the lip muscles during silent recitation tasks compared to rest, but no increase during the non-linguistic visualisation task [@Livesay1996]. An increase in the lip and forehead muscular activity has also been observed during induced rumination [@Nalborczyk2017]. Furthermore, this last study also suggested that speech-related muscle relaxation was slightly more efficient in reducing subjective levels of rumination than non speech-related muscle relaxation, suggesting that relaxing or inhibiting the speech muscles could disrupt rumination.

### Rumination

Rumination is a "class of conscious thoughts that revolve around a common instrumental theme and that recur in the absence of immediate environmental demands requiring the thoughts" [@Martin1996]. Despite the fact that depressed patients report positive metacognitive beliefs about ruminating, which is often seen as a coping strategy in order to regulate mood [e.g.,@Papageorgiou2001], rumination is known to significantly worsen mood [e.g., @Moberly2008;@Nolen-hoeksema1993], impair cognitive flexibility [e.g.,@Davis2000;@Lyubomirsky1998], and to lead toward pronounced social exclusion and more interpersonnal distress [@Lam2003]. Although partly visual, rumination is a predominantly verbal process [@Goldwin2012;@Mclaughlin2007] and can be considered as a maladaptive type of inner speech.

In a study on worry, another form of repetitive negative thinking, @Rapee1993 observed a *tendency* for articulatory suppression, but not for visuo-spatial tasks, to produce some interference with worrying. He concluded that worry involves the phonological aspect of the central executive of working memory. We further add that, since repeating a word seems to reduce the ability to worry, this study suggests that articulatory aspects are at play during worry. 

In this context, the question we addressed in this study is whether verbal rumination consists of purely abstract verbal representations or whether it is better described as a motor simulation of speech production, engaging the speech apparatus. If the latter hypothesis is correct, rumination experienced in verbal form (in contrast to a non-verbal form) should be disrupted by mouthing (i.e., silent articulation), and should not be disrupted by a control task that does not involve speech muscles (e.g., finger-tapping). Specifically, we thus sought to test the hypotheses that rumination could be disrupted by articulatory suppression (but not by finger-tapping), and that this disruption would be more pronounced when rumination is experienced in a verbal form than in a non-verbal form.

## Methods

In the *Methods* and *Data analysis* sections, we report how we determined our sample size, all data exclusions, all manipulations, and all measures in the study [@Simmons2012]. A pre-registered version of our protocol can be found on OSF: [https://osf.io/3bh67/](https://osf.io/3bh67/).

```{r, results = "hide", warning = FALSE}
####################################
# loading raw data
##########################

DFtotal <-
    read.csv("data/suppressiondata.csv", header = TRUE, sep = ",") %>%
    dplyr::rename(RUM = VAS)

# duplicating DFtotal for subsequent use
DF <- DFtotal

# number of people that were higher than the CES-D threshold (not included in the study)
depressive <- 26

# removing participants showing no effects of induction
ppt_induc <-
    which(DF$RUM[DF$Session == "Post-induction"] <= DF$RUM[DF$Session == "Baseline"])

for (i in 1:length(ppt_induc) ) DF <- DF[!DF$Participant == ppt_induc[i], ]

# keeping the entire uncentered dataframe as "DFclean0" for future use
DFclean0 <- DF

# centering predictors
DF[, 5:11] <- lapply(DF[, 5:11], function(x) as.numeric(scale(x, scale = TRUE) ) )

# keeping the entire dataframe as "DFclean" for future use
DFclean <- DF

# filtering post-motor Session and contrast-coding Session
DF <-
    DF %>%
    filter(!Session == "Post-motor") %>%
    mutate(Induction = ifelse(.$Session == "Baseline", -0.5, 0.5) )
```

### Sample

We originally planned for 128 participants to take part in the study. This sample size was set on the basis of results obtained by @Topolinski2009, who observed an effect size around $\eta_{p}^{2}=.06$. We expected a similar effect size for the current rumination disruption, since rumination can be conceived of as a subtype of inner speech^[In the original power calculations included in the OSF preregistration platform, we had inadequately specified the effect size in GPower, but we only realised this erroneous specification after the freezing of the preregistration on the OSF platform. Therefore, the current sample size slightly differs from the preregistered one.].

As we anticipated drop-out of participants due to our inclusion criteria (see below), a total of `r length(unique(DFtotal$Participant)) + depressive` undergraduate students in psychology from the Université Grenoble Alpes took part in this experiment, in exchange for course credits. They were recruited via mailing list, online student groups, and posters. Each participant provided a written consent and this study was approved by the local ethics committee (CERNI N° 2016-05-31-9). To be eligible, participants had to be between 18 and 35 years of age, with no history of motor, neurological, psychiatric, or speech-development disorders. All participants spoke French as their mother tongue. After each participant gave their written consent, they completed the Center for Epidemiologic Studies - Depression scale [CES-D; @Radloff1977a]. The CES-D is a 12-item questionnaire, validated in French [@Morin2011], aiming to assess the level of depressive symptoms in a subclinical population. Participants exceeding the threshold of clinical depressive symptoms [i.e., >23 for females and >17 for males; @Radloff1977a] were not included in the study for ethical reasons (N = `r depressive`).

To investigate articulatory suppression effects in the context of rumination, a successful induction of rumination is a prerequisite. Therefore, analyses were only conducted on participants who showed an effect of the rumination induction (i.e., strictly speaking, participants who reported more rumination after the induction than before). We thus discarded participants who did not show any increase in rumination level (N = `r length(unique(DFtotal$Participant) ) - length(unique(DF$Participant) )`, `r round((length(unique(DFtotal$Participant) ) - length(unique(DF$Participant) ) ) / length(unique(DFtotal$Participant) ) * 100, 2)`% of total sample). The final sample comprised `r length(unique(DF$Participant) )` participants (Mean age = `r mean(DF$Age)`, SD = `r sd(DF$Age)`, Min-Max = `r min(DF$Age)`-`r max(DF$Age)`, `r sum(DF$Gender=="F") / length(unique(DF$Session))` females).

### Material

The experiment was programmed with OpenSesame software [@Mathot2012] and stimuli were displayed on a DELL latitude E6500 computer screen.

#### Questionaires

To control for confounding variables likely to be related to the intensity of the induction procedure, we administered the French version of the Positive and Negative Affect Schedule [PANAS; @Watson1988], adapted to French by @Gaudreau2006. This questionnaire includes 20 items, from which we can compute an overall index of both positive (by summing the scores on 10 positive items, thereafter *PANASpos*) and negative affect (*PANASneg*) at baseline. This questionnaire was administered at baseline. In order to evaluate trait rumination, at the end of the experiment participants completed the short version of the Ruminative Response Scale [RRS-R, @Treynor2003], validated in French (Douilliez, Guimpel, Baeyens, & Philippot, *in preparation*). From this questionnaire, scores on two dimensions were analysed (*RRSbrooding* and *RRSreflection*).

#### Measures

Measures of state rumination were recorded using a Visual Analogue Scale (VAS) previously used in @Nalborczyk2017. This scale measured the degree of agreement with the sentence "At this moment, I am brooding on negative things" (translated from French), on a continuum between "Not at all" and "A lot" (afterwards coded between 0 and 100). This scale is subsequently referred to as the *RUM* scale. It was used three times in the experiment, at baseline (after training but before the experiment started), after rumination induction, and after a motor task.

Additionally, participants answered questions about the modality of the thoughts that occurred while performing the motor task. This last questionnaire consisted of one question evaluating the occurrence frequency of different modalities of inner thoughts (e.g., visual imagery, verbal thoughts, music). Then, a verbal/non-verbal ratio (i.e., the score on the verbal item divided by the mean of the score on the non-verbal items) was computed, hereafter referred to as the *Verbality* continuous predictor (this scale is available online: [https://osf.io/3bh67/](https://osf.io/3bh67/)).

#### Tasks

In the first part of the experiment, ruminative thoughts were induced using a classical induction procedure. Then a motor task was executed. Participants were randomly allocated to one of two conditions. In the *Mouthing* condition, the task consisted of repetitively making mouth opening-closing movements at a comfortable pace. This condition was selected as it is commonly used in articulatory suppression studies [e.g.,@Baddeley1984]. As a control, a finger-tapping condition was used (the *Tapping* condition), that consisted of tapping on the desk with the index finger of the dominant hand at a comfortable pace.

Although finger-tapping tasks are generally considered as good control conditions when using speech motor tasks, since they are comparable in terms of general attentional demands, it may be that orofacial gestures are intrinsically more complex than manual gestures [i.e., more costly, @Emerson2003]. To discard the possibility that orofacial gestures (related to the *Mouthing* condition) would be cognitively more demanding than manual ones (related to the *Tapping* condition), we designed a pretest experiment in order to compare the two interference motor tasks used in the main experiment. Results of this control experiment showed no difference on reaction times during a visual search task between the two interference tasks (i.e., mouthing and finger-tapping). Full details are provided in Appendix A.

### Procedure

The experiment took place individually in a quiet and dimmed room. The total duration of the session ranged between 35min and 40min. Before starting the experiment, participants were asked to perform the motor task during 1 min, while following a dot moving at a random pace on the screen in front of them. This task was designed to train the participants to perform the motor task adequately. Following this training and after describing the experiment, the experimenter left the room and each participant had to fill-in a baseline questionnaire (adaptation of PANAS, see above) presented on the computer screen. Baseline state rumination was then evaluated using the *RUM* scale. The whole experiment was video-monitored using a Sony HDR-CX240E video camera, in order to check that the participants effectively completed the task.

#### Rumination induction

Rumination induction consisted of two steps. The first step consisted of inducing a negative mood in order to enhance the effects of the subsequent rumination induction. Participants were asked to recall a significant personal failure experienced in the past five years. Then, participants were invited to evaluate the extent to which this memory was "intense for them" on a VAS between "Not at all" and "A lot", afterwards coded between 0 and 100, and referred to as *Vividness*.

The second step consisted of the rumination induction proper. We used a French translation of the @Nolen-hoeksema1993 rumination induction procedure. Participants had to read a list of 44 sentences related to the meaning, the causes and the consequences of their current affective or physiological state. Each phrase was presented on a computer screen for 10 seconds and the total duration of this step was 7 minutes and 20 seconds. State rumination was then evaluated again using the same VAS as the one used at baseline (*RUM*).

#### Motor task {#proc_supp}

After the rumination induction, participants were asked to continue to think about "the meaning, causes, and consequences" of their feelings while either repetitively making mouth movements (for participants allocated in the "Mouthing" condition) or finger-tapping with the dominant hand for five minutes (for participants allocated in the "Tapping" condition). Afterwards, state rumination was again evaluated using the *RUM* scale.

In order to evaluate trait rumination, participants completed the short version of the RRS (see above). Then were filled in the questionnaire on the modality of the thoughts that occurred while performing the motor task (see above). Figure \@ref(fig:diagram) summarises the full procedure.

```{r, results = "hide", warning = FALSE}
#####################################
# MLMs for induction
##############################

M1 <- lme4::lmer(RUM ~ Induction + (1|Participant), REML = FALSE, DF)
M2 <- lme4::lmer(RUM ~ Induction + PANASpos + (1|Participant), REML = FALSE, DF)
M3 <- lme4::lmer(RUM ~ Induction + PANASneg + (1|Participant), REML = FALSE, DF)
M4 <- lme4::lmer(RUM ~ Induction + Induction:Vividness + (1|Participant), REML = FALSE, DF)
M5 <- lme4::lmer(RUM ~ Induction + PANASpos + Induction:Vividness + (1|Participant), REML = FALSE, DF)
M6 <- lme4::lmer(RUM ~ Induction + PANASneg + Induction:Vividness + (1|Participant), REML = FALSE, DF)
M7 <- lme4::lmer(RUM ~ Induction + PANASpos + PANASneg + Induction:Vividness + RRSreflection + (1|Participant), REML = FALSE, DF)
M8 <- lme4::lmer(RUM ~ Induction + PANASpos + PANASneg + Induction:Vividness + RRSbrooding + (1|Participant), REML = FALSE, DF)
M9 <- lme4::lmer(RUM ~ Induction + PANASpos + PANASneg + Induction:Vividness + RRSbrooding + RRSreflection + (1|Participant), REML = FALSE, DF)

#############################################
# AIC model comparison table
#####################################

AICtab1 <- aictab(
    list(M1, M2, M3, M4, M5, M6, M7, M8, M9),
    modnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9")
    )

AICtab1 <- AICtab1[, c(1:4, 6)]

# appending to AICtab2 the marginal and conditionnal R-squared
AICtab1$R2m <- t(sapply(AICtab1$Modnames,
    function(x) r.squaredGLMM(get(as.character(x) ) ) ) )[, 1]

AICtab1$R2c <- t(sapply(AICtab1$Modnames,
    function(x) r.squaredGLMM(get(as.character(x) ) ) ) )[, 2]

table <- data.frame(AICtab1[, 2:5])

rownames(table) <- c(
    "$Int+Ind+PANASpos+PANASneg+Ind:Viv+RRSbro$",
    "$Int+Ind+PANASpos+PANASneg+Ind:Viv+RRSref$",
    "$Int+Ind+PANASpos+PANASneg+Ind:Viv+RRSbro+RRSref$",
    "$Int+Ind+PANASneg+Ind:Viv$",
    "$Int+Ind+PANASneg$",
    "$Int+Ind+PANASpos+Ind:Viv$",
    "$Int+Ind+PANASpos$",
    "$Int+Ind+Ind:Viv$",
    "$Int+Ind$"
    )

colnames(table) <- c("$K$", "$AICc$", "$\\Delta_{AICc}$", "$Weight$")

#############################################################
# Preparing summary table for the best model
#####################################################

bestmodel <- get(as.character(AICtab1[1, 1] ) )

parameters1 <-
    bestmodel %>%
    tidy(effects = "fixed", conf.int = TRUE) %>%
    dplyr::select(-statistic) %>%
    `colnames<-`(c("", "Est", "SE", "Lower", "Upper") )

##################################################
# Preparing data for plot
###################################

DF2 <-
    DFclean %>%
    group_by(Session, Condition) %>%
    summarise(Lower = CI(RUM)[3], Mean = CI(RUM)[2], Upper = CI(RUM)[1]) %>%
    data.frame
```

```{r diagram, dev = "pdf", fig.pos = "H", cache = TRUE, fig.align = "center", fig.width = 6, fig.height = 6, fig.cap = "Timeline of the experiment, from top to bottom."}
# graph LR (left-right) or graph TD (top-down)

g <- mermaid("
    graph TD;

    a0(<center>Motor training</center>) --> a;

    a(<center>Baseline measures <br> PANAS & RUM</center>) -->|<i>Mood induction</i>\ |a1(<center>Vividness control </center>);

    a1-->|<i>Rumination induction</i>\ |b(<center>Post-Induction measures <br> RUM</center>);

    b --> c(<center>Mouthing</center>);
    b --> d(<center>Tapping</center>);

    c --> e(Post-activity measures <br> RUM);

    b --> |<i>Session \ ' </i>\ |e;

    d --> e(<center>Post-activity measures <br> RUM</center>);
    e --> f(<center>Verbality & Trait rumination: RRS-R</center>);
    f --> g(<center>Debriefing</center>);

    style a0 fill:lightgrey, stroke:black, stroke-width:2px;
    style a1 fill:lightgrey, stroke:black, stroke-width:2px;
    style a fill:lightgrey, stroke:black, stroke-width:2px;
    style b fill:lightgrey, stroke:black, stroke-width:2px;
    style c fill:lightgrey, stroke:black, stroke-width:2px;
    style d fill:lightgrey, stroke:black, stroke-width:2px;
    style e fill:lightgrey, stroke:black, stroke-width:2px;
    style f fill:lightgrey, stroke:black, stroke-width:2px;
    style g fill:lightgrey, stroke:black, stroke-width:2px;

    ")

plotly::export(g, file = "assets/mermaid.pdf")
```

### Data analysis

Statistical analyses were conducted using R version 3.4.3 [@R-base], and are reported with the `papaja` [@R-papaja] and `knitr` [@R-knitr] packages.

#### Rumination induction

We centered and standardised each predictor in order to facilitate the interpretation of parameters. Data were then analysed using *Induction* (2 modalities, before and after induction, contrast-coded) as a within-subject categorical predictor and *RUM* as a dependent variable in a multilevel linear model (MLM). Data were fitted using the `lmer` function, within the `lme4` package [@lme4]. This model was compared with more complex models including effects of control variables, such as baseline affect state (*PANAS* scores) or the vividness of the memory chosen during the induction (*Vividness* score). Models were compared using the corrected Akaike Information Criterion (AICc) and evidence ratios [@Burnham2002;@Burnham2011;@Hegyi2011]. AICc provides a relative measure of predictive accuracy of the models (the AIC is an approximation of the out-of-sample deviance of a model) and balances underfitting and overfitting by sanctioning models for their number of parameters. We computed the difference between the best (lower) and other AICcs with $\Delta_{AICc}=AICc_{i}-AICc_{min}$ and then expressed the weight of a model as:

$$w_{i}=\dfrac{exp(-\Delta_{i}/2)}{\sum_{r=1}^{R}exp(-\Delta_{r}/2)}$$

From there, we computed evidence ratios (ERs) as the ratios of weights: $ER_{ij} = \dfrac{w_{i}}{w_{j}}$, where $w_{i}$ and $w_{j}$ are the Akaike weights of models $i$ and $j$, respectively. These weights can be interpreted as the probability of the model being the best model in terms of out-of-sample prediction [@Burnham2002]. Instead of reporting null-hypothesis tests for our MLMs, we report 95% confidence intervals for the constant effects estimates^[These may be interpreted as tests of significance: if the confidence interval for an estimated parameter does not contain zero, this estimate may be considered significant at $\alpha$ <.05.].

Whereas the use of AICc is appropriate for model comparison and selection, it tells us nothing about the absolute fit of the model. To estimate this fit, we computed two types of $R^2$ for MLMs using the `MuMIn` package [@MuMIn]. The first, called the marginal $R^2$ ($R^2_{marg.}$), estimates the proportion of variance accounted for by the constant effects, whereas the second, called the conditional $R^2$ ($R^2_{cond.}$), estimates the proportion of variance accounted for by the constant and the varying effects taken together [@Getz2015;@Johnson2014;@Nakagawa2013].

#### Articulatory suppression effects

Data were analysed in the same fashion as in the first part of the experiment, using *Session* (2 modalities, before and after motor activity, contrast-coded) as a within-subject categorical predictor, and *Condition* (2 modalities, Mouthing and Tapping) as a between-subject categorical predictor and *RUM* as a dependent variable, in a MLM.

## Results

### Correlation matrix between main predictors and control variables

```{r, results = "hide", warning = FALSE}
# keeping only one unique line per participant to compute correlations
DFcorr <- DFclean[DFclean$Session == "Baseline", ]
```

In order to prevent multicollinearity, we estimated the correlation between each pair of continuous predictors. Figure \@ref(fig:correxp1) displays these correlations along with the marginal distribution of each variable. The absence of strong correlations ($r > 0.8$) between any of these variables suggests that they can each be included as control variables in the following statistical models. Summary statistics (mean and standard deviation) for all these variables can be found in Table \@ref(tab:sumstat).

```{r correxp1, fig.pos = "H", fig.width = 10, fig.height = 10, fig.cap = "Diagonal: marginal distribution of each variable. Panels above the diagonal: Pearson's correlations between main continuous predictors, along with 95% CIs. The absolute size of the correlation coefficient is represented by the size of the text (lower coefficients appear as smaller). Panels below the diagonal: scatterplot of each variables pair."}
#################################################
# correlation matrix
###################################

my_custom_cor <- function(data, mapping, color = I("grey50"), ...) {
    
    # get the x and y data to use the other code
    
    x <- GGally::eval_data_col(data, mapping$x)
    y <- GGally::eval_data_col(data, mapping$y)
    
    ct <- cor.test(x, y)
    
    r <- unname(ct$estimate)
    rt <- format(r, digits = 2)[1]

    ci <- unname(ct$conf.int)
    cit <- format(ci, digits = 2)
    
    range <- max(abs(as.numeric(ci) ) ) - min(abs(as.numeric(ci) ) )
        
    # helper function to calculate a useable size
    
    percent_of_range <- function(percent, range) {
        
        percent * diff(range) + min(range, na.rm = TRUE)
      
    }
      
    # plot the correlation value
    
    ggally_text(
        label = paste0(as.character(rt), "\n [", cit[1], ", ", cit[2], "]"), 
        mapping = aes(),
        xP = 0.5, yP = 0.5, 
        size = I(percent_of_range(abs(r), c(4, 9) ) ),
        color = "grey40",
        ...) + 
        # remove all the background stuff and wrap it with a dashed line
        theme_classic() + 
        theme(
            panel.background = element_rect(
                color = color, 
                linetype = "longdash"
            ), 
            axis.line = element_blank(), 
            axis.ticks = element_blank(), 
            axis.text.y = element_blank(), 
            axis.text.x = element_blank()
        )
}

DFcorr %>%
    # selecting relevant variables
    select(PANASpos, PANASneg, Vividness, RRSbrooding, RRSreflection, Verbality) %>%
    # centering variables
    mutate_all(funs(as.numeric(scale(.) ) ) ) %>%
    # plotting the correlation matrix
    GGally::ggpairs(
        lower = list(continuous = GGally::wrap("points", color = "grey30", shape = 16) ),
        upper = list(continuous = my_custom_cor)
        ) +
    theme_bw(base_size = 14)
```

```{r summary, results = "asis"}
summary_stat <- 
    DFclean0 %>%
    mutate_if(is.factor, funs(as.character) ) %>% 
    select(Condition, Session, RUM:Age) %>% 
    group_by(Condition, Session) %>%
    gather(Variable, Value, RUM:Age) %>% 
    group_by(Condition, Session, Variable) %>% 
    dplyr::summarise(
        APA_descriptive_statistics = glue::glue("{mean} ({sd})",
            mean = round(mean(Value), 2), sd   = round(sd(Value), 2) )
        ) %>% 
    reshape2::dcast(
        Condition + Session ~ Variable,
        value.var = "APA_descriptive_statistics"
        ) %>%
    select(Condition, Session, RUM, Age:Vividness) %>%
    data.frame()

#write.csv2(summary_stat, "summary_statistics.csv", sep = "\t", row.names = FALSE)
```

```{r sumstat, results = "asis"}
summary_stat[summary_stat$Session %in% c("Post-induction", "Post-motor"), 4:11] <- "-"

summary_stat2 <- 
    summary_stat %>%
    t %>%
    data.frame(stringsAsFactors = FALSE) %>%
    rownames_to_column(., var = "Variables")

summary_stat3 <- summary_stat2[c(-1, -2), ]

summary_stat3 %>%
    kable(
    #placement = "H",
    align = c("l", rep("c", 6) ),
    caption = "Descriptive statistics (mean and standard deviation) of each recorded variable, for the final sample of participants that were included in the study.",
    #small = TRUE,
    #landscape = TRUE,
    #col_spanners = list(`Mouthing` = c(2, 4), `Tapping` = c(5, 7) ),
    escape = FALSE,
    row.names = FALSE,
    col.names = c("Variables", "Baseline", "Post-induction", "Post-motor", "Baseline", "Post-induction", "Post-motor")
    )
```

### Rumination induction

```{r effect_size1}
# computing the effect size (Cohen's d) of the induction

induction_d_av <-
    effsize::cohen.d(
        d = DF$RUM, f = factor(DF$Session),
        paired = FALSE, pooled = TRUE
        )

d_induction <- induction_d_av$estimate %>% as.numeric * ( - 1)
d_induction_lower <- induction_d_av$conf.int[2] %>% as.numeric * ( - 1)
d_induction_upper <- induction_d_av$conf.int[1] %>% as.numeric * ( - 1)
```

To examine the efficiency of the induction procedure (i.e., the effect of *Induction*) while controlling for the other variables (i.e., *Vividness*, *RRSbrooding*, *RRSreflection*, *PANASpos*, and *PANASneg*), we then compared the parsimony of models containing main constant effects and a varying intercept for *Participant*. Model comparison showed that the best model (in the sense of the lowest AICc model) was the model including *Induction*, *PANASpos*, *PANASneg*, *RRSbooding*, and an interaction term between *Induction* and *Vividness* as predictors (see Table \@ref(tab:compexp1)). Fit of the best model was moderate as marginal $R^{2}$ was of `r r.squaredGLMM(bestmodel)[1]` while conditional $R^{2}$ was of `r r.squaredGLMM(bestmodel)[2]`.

```{r compexp1, results = "asis"}
kable(
    table,
    #placement = "H",
    align = c("l","c","c","c","c"),
    caption = "Comparison of models, ordered by AICc relative to the model with the lowest AICc.",
    #note = "$K$ is the number of estimated parameters in the model. $Int$ = Intercept, $Ind$ = Induction, $Viv$ = Vividness, $RRSbro$ = RRSbrooding, $RRSref$ = RRSreflection. All models include a varying intercept for Participant.",
    #small = TRUE,
    #landscape = TRUE,
    format.args = list(
        digits = c(0, 2, 2, 3),
        margin = 2,
        decimal.mark = ".", big.mark = ""),
    escape = FALSE) 
```

Constant effect estimates for the best model are reported in Table \@ref(tab:paramexp1). Based on these values, it seems that *Induction* (i.e., the effects of the rumination induction) increased *RUM* scores by approximately 26 points in average (\(d_{av} =\) `r round(d_induction, 3)`, 95% CI [`r round(d_induction_lower, 3)`, `r round(d_induction_upper, 3)`]). The main negative effect of *PANASneg* and the main positive effects of *PANASpos* indicate, respectively, that negative baseline mood was associated with higher levels of rumination while positive baseline mood was associated with lower levels of self-reported rumination.

```{r paramexp1, results = "asis"}
kable(
    parameters1,
    #placement = "H",
    align = c("l","c","c","c","c"),
    caption = "Coefficient estimates (Est), standard errors (SE), and 95% CIs (Lower, Upper).",
    #note = "This table reports constant effects estimates from the best model. As all predictors were centered to the mean for analysis, these coefficients approximate coefficients from simpler models.",
    #small = TRUE,
    digits = 3,
    escape = TRUE
    ) 
```

Higher scores on *Vividness* were associated with higher increase in self-reported rumination after induction, as revealed by the positive coefficient of the interaction term. This suggests that participants who recalled a more vivid negative memory tended to show a higher increase in rumination after the induction procedure than participants with a less vivid memory.

## Articulatory suppression effects on induced rumination

```{r, results = "hide", warning = FALSE}
# extracting baseline RUM score by participant
rum_baseline <- DFclean %>% filter(Session == "Baseline") %>% select(RUM)

DF <-
    DFclean %>%
    mutate(RUMbaseline = rep(rum_baseline$RUM, each = n_distinct(.$Session) ) ) %>%
    filter(!Session == "Baseline") %>%
    mutate(
        Session = ifelse(.$Session == "Post-induction", -0.5, 0.5),
        Condition = ifelse(.$Condition == "Mouthing", -0.5, 0.5),
        RUMbaseline = scale(RUMbaseline, scale = TRUE)
        )

################################################################
# MLMs for experiment 2
################################################

M1 <- lme4::lmer(RUM ~ 1 +
        (1|Participant), REML = FALSE, DF)
M2 <- lme4::lmer(RUM ~ 1 + Session +
        (1|Participant), REML = FALSE, DF)
M3 <- lme4::lmer(RUM ~ 1 + Session + Condition +
        (1|Participant), REML = FALSE, DF)
M4 <- lme4::lmer(RUM ~ 1 + Session + Condition + Session:Condition +
        (1|Participant), REML = FALSE, DF)
M5 <- lme4::lmer(RUM ~ 1 + Session + Condition + Session:Condition + Session:Condition:Verbality +
        (1|Participant), REML = FALSE, DF)
M6 <- lme4::lmer(RUM ~ 1 + Session + Condition +
        RUMbaseline + RRSbrooding + RRSreflection + PANASneg + (1|Participant), REML = FALSE, DF)
M7 <- lme4::lmer(RUM ~ 1 + Session + Condition + Session:Condition +
        RUMbaseline + RRSbrooding + RRSreflection + PANASneg + (1|Participant), REML = FALSE, DF)
M8 <- lme4::lmer(RUM ~ 1 + Session + Condition + Session:Condition + Session:Condition:Verbality +
        RUMbaseline + RRSbrooding + RRSreflection + PANASneg + (1|Participant), REML = FALSE, DF)

##########################################
# AICtab
###############################

# model comparison table
AICtab2 <- aictab(list(M1, M2, M3, M4, M5, M6, M7, M8), modnames = c("M1","M2","M3","M4","M5","M6","M7","M8") )
AICtab2 <- AICtab2[, c(1:4, 6)]

# appending to AICtab2 the marginal and conditionnal R-squared
AICtab2$R2m <- t(sapply(AICtab2$Modnames,
    function(x) r.squaredGLMM(get(as.character(x) ) ) ) )[, 1]

AICtab2$R2c <- t(sapply(AICtab2$Modnames,
    function(x) r.squaredGLMM(get(as.character(x) ) ) ) )[, 2]

AICtab2$AICcWt <- round(AICtab2$AICcWt, digits = 3)

table2 <- data.frame(AICtab2[, 2:5])

rownames(table2) <- c(
    "$Int+Session+Cond+RUMb+PANASneg+RRSbro+RRSref$",
    "$Int+Session+Cond+Session:Cond+RUMb+PANASneg+RRSbro+RRSref$",
    "$Int+Session+Cond+Session:Cond+Session:Cond:Verb+RUMb+PANASneg+RRSbro+RRSref$",
    "$Int+Session$",
    "$Int+Session+Cond$",
    "$Int+Session+Cond+Session:Cond$",
    "$Int+Session+Cond+Session:Cond:Verb$",
    "$Int$"
    )

colnames(table2) <- c("$K$", "$AICc$", "$\\Delta_{AICc}$", "$Weight$")

###############################################################
# Preparing summary table for the best model
###################################################

#bestmodel2 <- get(as.character(AICtab2[1, 1]) )
bestmodel2 <- M7

parameters2 <-
    tidy(bestmodel2, conf.int = TRUE) %>%
    dplyr::select(-statistic,-group) %>%
    `colnames<-`(c("", "Est", "SE", "Lower", "Upper") )

# computing the intra-class correlation for varying intercept
ICC <- parameters2[9, 2]^2 / (parameters2[9, 2]^2 + parameters2[10, 2]^2)

# computing the effect size (Cohen's d average) for each motor task
d_av_mouthing <-
    effsize::cohen.d(
        d = DF$RUM[DF$Condition==-0.5], f = factor(DF$Session[DF$Condition==-0.5]),
        paired = FALSE, pooled = TRUE
        )

d_mouthing <- d_av_mouthing$estimate %>% as.numeric * ( - 1)
d_mouthing_lower <- d_av_mouthing$conf.int[2] %>% as.numeric * ( - 1)
d_mouthing_upper <- d_av_mouthing$conf.int[1] %>% as.numeric * ( - 1)

d_av_tapping <-
    effsize::cohen.d(
        d = DF$RUM[DF$Condition==0.5], f = factor(DF$Session[DF$Condition==0.5]),
        paired = FALSE, pooled = TRUE
        )

d_tapping <- d_av_tapping$estimate %>% as.numeric * ( - 1)
d_tapping_lower <- d_av_tapping$conf.int[2] %>% as.numeric * ( - 1)
d_tapping_upper <- d_av_tapping$conf.int[1] %>% as.numeric * ( - 1)
```

We then examined the effect of the two motor tasks (articulatory suppression and finger-tapping) on *RUM*, while controlling for other variables (i.e., *Vividness*, *RRSbrooding*, *RRSreflection*, *Verbality*, *PANASpos*, and *PANASneg*). Given the group differences on *RUM* score at baseline (i.e., after training), we also included this score as a control variable in our models, as the *RUMb* variable. Based on our hypotheses, we expected that the model comparison would reveal a three-way interaction between *Session*, *Condition* and *Verbality*. However, the best model identified by AICc model comparison did not include this interaction as a constant effect. Nonetheless, the best model was only slightly better than the model including the three-way interaction (the second model in Table \@ref(tab:compexp2)), as the best model was only `r AICtab2$AICcWt[AICtab2$Modnames=="M6"] / AICtab2$AICcWt[AICtab2$Modnames=="M7"]` more *credible* than the interaction model. As our goal is precise estimation of effects rather than dichotomic decision about the presence or absence of an effect, we chose to present the estimations of the second model as well. Fit of this model was moderate as marginal $R^{2}$ was `r r.squaredGLMM(bestmodel2)[1]` while conditional $R^{2}$ was `r r.squaredGLMM(bestmodel2)[2]`.

```{r compexp2, results = "asis"}
kable(
    table2,
    #placement = "H",
    align = c("l","c","c","c","c"),
    caption = "Comparison of models, ordered by AICc relative to the model with the lowest AICc.",
    #note = "$K$ is the number of estimated parameters in the model. $Int$ = Intercept, $Cond$ = Condition, $RUMb$ = RUM baseline score, $Verb$ = Verbality, $RRSbro$ = RRSbrooding, $RRSref$ = RRSreflection. All models include a varying intercept for Participant.",
    #small = TRUE,
    #landscape = TRUE,
    format.args = list(
        digits = c(0, 2, 2, 3),
        margin = 2,
        decimal.mark = ".", big.mark = ""
        ),
    escape = FALSE
    )
```

Parameter values of the best model for the second part of the experiment are reported in Table \@ref(tab:paramexp2). Based on these values, it seems that self-reported rumination decreased after both motor tasks (the coefficient for *Session* is negative), but this decrease was substantially larger in the *Mouthing* condition (\(d_{av} =\) `r round(d_mouthing, 3)`, 95% CI [`r round(d_mouthing_lower, 3)`, `r round(d_mouthing_upper, 3)`]) than in the *Tapping* condition (\(d_{av} =\) `r round(d_tapping, 3)`, 95% CI [`r round(d_tapping_lower, 3)`, `r round(d_tapping_upper, 3)`]), as can be read from the coefficient of the interaction term between *Session* and *Condition* (*Est* = 5.965, *SE* = 4.320, *95% CI* [-2.502, 14.433]). However, the large uncertainty associated with this result (as expressed by the width of the confidence interval) warrants a careful interpretation of this result, that should be considered as suggestive evidence, rather than conclusive evidence.

The large variation between participants can be appreciated by computing the *intra-class correlation* (ICC), expressed as $\sigma_{intercept}^{2}/(\sigma_{intercept}^{2}+\sigma_{residuals}^{2})$. For the best model, the ICC is equal to `r round(ICC, 4)`, indicating that `r round(100 * ICC, 2)`% of the variance in the outcome that remains after accounting for the effect of the predictors, is attributable to systematic inter-individual differences.

```{r paramexp2, results = "asis"}
kable(
    parameters2,
    #placement = "H",
    align = c("l","c","c","c","c"),
    caption = "Estimates (Est), standard errors (SE), and 95% CIs (Lower, Upper).",
    #note = "This table reports estimates from the best model. As all predictors were centered to the mean for analysis, these coefficients approximate coefficients from simpler models. Standard errors and confidence intervals are not reported for variance components as their skewed sampling distribution is known to be poorly approximated by classical estimates.",
    #small = TRUE,
    digits = 3,
    escape = TRUE,
    format.args = list(
        na_string = "-"
        )
    )
```

Figure \@ref(fig:plotexp1) shows the evolution of the mean *RUM* scores all through the experiment according to each session (Baseline, Post-induction, Post-motor) and *Condition* (Mouthing, Tapping). This figure reveals important inter-individual variability, in all conditions. After the rumination induction, *RUM* score increased in both groups, and decreased after the motor task, with a stronger decrease in the *Mouthing* condition.

```{r plotexp1, fig.pos = "H", fig.width = 8, fig.height = 6, fig.cap = "Mean RUM score by Session and Condition, along with violin plots and individual data. Error bars represent 95% CIs. The horizontal bar inside the violin plots represents the median of the conditional distribution."}

pd <- position_dodge(0.9)

DFclean %>%
    ggplot(aes(x = Session, y = RUM, colour = Condition, fill = Condition) ) +
    # violin plots
    geom_violin(
        scale = "count", alpha = 0.1,
        position = pd,
        #adjust = 0.8,
        draw_quantiles = 0.5,
        show.legend = FALSE) +
    # plotting individual data points
    geom_dotplot(
        stackdir = "center",
        binaxis = "y",
        position = pd,
        binwidth = 1,
        dotsize = 1.5,
        alpha = 0.2) +
    stat_summary(
        fun.y = mean,
        geom = "line", size = 1,
        aes(group = Condition),
        position = pd,
        show.legend = FALSE) +
    # plotting means
    stat_summary(
        fun.y = "mean", geom = "point", shape = 16, size = 5,
        position = pd,
        show.legend = TRUE) +
    # plotting confidence intervals
    stat_summary(
        fun.data = mean_cl_normal,
        geom = "errorbar", size = 1, width = 0,
        fun.args = list(mult = 1.96),
        show.legend = FALSE,
        position = pd) +
    # grey scale
    scale_colour_grey(start = 0.3, end = 0.7) +
    scale_fill_grey(start = 0.3, end = 0.7) +
    labs(title = "", x = "", y = "Mean RUM score") +
    theme_bw(base_size = 14)
```

Figure \@ref(fig:plotverbal) shows the effects of *Verbality* on the relative change (i.e., after - before) in self-reported rumination after both motor activities (i.e., *Mouthing* and *Tapping*). As *Verbality* was centered before analysis, its score cannot be interpreted in absolute terms. However, a high score on this index indicates more verbal than non-verbal (e.g., visual images, non-speech sounds) thoughts, while a low score indicates more non-verbal than verbal thoughts. Contrary to our predictions but consistent with the model comparison, this figure depicts a similar relationship between *Verbality* and the change in *RUM* score (between before and after the motor task), according to the Condition.

```{r plotverbal, fig.pos = "H", fig.width = 8, fig.height = 6, fig.cap = "Mean RUM relative change after motor activity, as a function of the degree of Verbality, in the mouthing (the dark grey dots and regression line) and finger tapping (the light grey dots and regression line) conditions."}

differential <- function(x) {
    
    x$RUM <- rep(x$RUM[DF4$Session == "Post-motor"] - x$RUM[DF4$Session == "Post-induction"], each = 2)
    
}

DF4 <- DFclean[!DFclean$Session == "Baseline", ]
DF4$RUM <- differential(DF4)
DF4 <- DF4[!DF4$Session == "Post-induction", ]

DF4 %>%
    ggplot(
        aes(
            x = Verbality, y = RUM, group = factor(Condition),
            color = Condition, fill = Condition)
        ) +
    scale_color_grey(start = 0.3, end = 0.7) +
    scale_fill_grey(start = 0.3, end = 0.7) +
    geom_hline(
        yintercept = 0, linetype = 3,
        alpha = 1) +
    geom_point(
        size = 3, shape = 20,
        alpha = 0.8,
        show.legend = TRUE) +
    geom_smooth(
        method = "lm",
        se = FALSE,
        size = 1.5,
        fullrange = FALSE,
        show.legend = FALSE) +
    labs(y = "RUM relative change") +
    theme_bw(base_size = 14)
```

## Discussion

The purpose of the current study was to investigate the effects of articulatory suppression on induced verbal rumination. We predicted that if verbal rumination, which can be construed as a type of inner speech, does involve the mental simulation of overt speech production, its generation should be disrupted by articulatory suppression, but not by finger tapping. This prediction was not strictly corroborated by the data, as we observed a decrease of self-reported rumination after both types of motor activities (see Figure \@ref(fig:plotexp1) and Table \@ref(tab:compexp2)), with a somewhat stronger decrease in the Mouthing condition. In the following, we examine the validity of our methods and discuss interpretations of our results. Finally, we formulate how subsequent research should address this kind of question and suggest alternative ways to test the above mentioned hypothesis. We begin by discussing the results of the rumination induction procedure.

### Rumination induction

It is noteworthy that `r round((length(unique(DFtotal$Participant) ) - length(unique(DF$Participant) ) ) / length(unique(DFtotal$Participant) ) * 100, 2)`% of the total sample of participants who were recruited did not respond to this induction, and were therefore not included in the analyses. Moreover, as reported in Table \@ref(tab:paramexp1), it seems that the *Vividness* of the memory chosen by the participant during the mood induction was moderating the effect of the rumination induction. In other words, the more vivid (i.e., the more "intense") the memory, the more successful the rumination induction was. This highlights the fact that this aspect should be carefully controlled each time a mood induction is used in order to foster subsequent repetitive negative thinking.

Moreover, we observed a group difference of approximately 7.5 points in the average *RUM* score at baseline. This difference might be explained by motor training, which took place before baseline measurement of state rumination. During this training, participants had to perform the motor task (either finger-tapping or mouthing) in front of a screen on which a white dot was moving randomly on a black screen, for 1 min. During the task, the experimenter stayed in the room (out of the participant's sight) to check that participants were performing the motor task adequately. Being an unusual and potentially embarrassing motor activity, mouthing might have been an higher source of stress for the participants, as compared to the more common activity of finger-tapping. This group difference in baseline state rumination subsisted after the induction, as the group difference after the induction is of approximately 8 points (see full dataset and summary statistics in the [supplementary materials](#supp)).

### Articulatory suppression effects

In the following section, we discuss in more depth the results of the second part of the study, which aimed at comparing the effects of articulatory suppression and finger-tapping on self-reported rumination.

First, it is important to examine whether our failure to detect the predicted interaction could come from a lack of statistical power. We planned 128 participants in order to reach a power of .80 for a targeted effect size of $\eta_{p}^{2}=.06$. As explained above, out of the 184 recruited participants, only 106 could be included in the study. With 106 participants, the a priori power was approximately of .70, which is much higher than the median power in typical psychological studies.

Second, it is important to acknowledge that despite the absence of the predicted difference between the two conditions in their influence on the level of self-reported rumination (i.e., *RUM*), both activities did lead, on average, to a decrease in self-reported rumination of approximately 6 points on the VAS (as indicated by the slope for *Session* in Table \@ref(tab:paramexp2)). This decrease might be interpreted in at least two ways. First, it might be explained by the simple exposition to the VAS and by compliance effects. When asked to rate their level of rumination again after five minutes of motor activity, some participants might be prompted to indicate a lower level of rumination than before the motor task. But compliance effects could similarly lead participants to consider the motor task as irritating, and therefore as prone to rumination increase. Some participants could therefore also be biased towards indicating a higher level of rumination after the motor task. Second, it might be considered that this decrease reflects a genuine decrease in rumination. In the following, we adopt the latter perspective and discuss explanations for the weak difference between the two conditions.

#### Effect of the rumination quality (verbality)

Our prediction was that rumination in verbal form would be more disrupted by mouthing than rumination in non-verbal form, while both kinds of rumination would not be disrupted (or similarly disrupted) by finger-tapping. In other words, we hypothesised a three-way interaction, between the effect of time (i.e., *Session*), *Condition*, and *Verbality*. In the following, we discuss the absence of this interaction. Then, we focus on the weak difference between the two conditions (omitting *Verbality*), and discuss some explanations for this weak difference.

First, the absence of the three-way interaction might come from a difficulty for the participants to have clear introspective access to the ruminative thoughts they experienced during the experiment. For instance, we know that introspective description of inner speech differs considerably, between people trained to regularly report on their episodes of inner speaking, and people without such training [e.g.,@Hurlburt2013]. Moreover, as the *Verbality* questionnaire was presented at the end of the experiment, one cannot exclude that it was partly contaminated by recall, which, when done verbally, has been shown to artificially increase the subjective verbality index [@Hurlburt2011].

#### Difference between motor conditions

Leaving the self-reported quality of rumination aside, we now turn to a discussion of the weak difference between the two conditions. We think this result can be explained in at least two non-exclusive ways. First, we could argue that the decrease observed in both conditions was due to an unexpected effect of finger-tapping on rumination. Second, we could argue that the effect of the articulatory suppression was somehow weaker than expected. In the following, we provide arguments and explanations for each of these possibilities.

Steady finger-tapping is usually considered as a relevant control condition for evaluating articulatory suppression, since it specifically recruits the hand motor system and should not interfere with the oral motor system, while being comparable in terms of general attentional demands [e.g.,@Gruber2001;@Logie1987]. However, using more complex rhythmic patterns of finger-tapping, @Saito1994 observed a fade-out of the phonological similarity effect in a verbal memory task with spoken recall, when subjects were asked to tap with either their right (dominant) or left hand, while the phonological similarity effect was conserved in the control condition (no tapping). The author concluded that a complex rhythmic tapping task can suppress the activity of the articulatory control process, by *suppressing* the running of speech motor programs [@Saito1994, page 185]. More specifically, he suggested that complex, non-automatised, rhythmic finger tapping could use speech motor programs, which are useful to control speech prosody, and therefore can deal with rhythmic activity. We further suggest that a novel complex rhythmic task might require silent verbalisation and, therefore, might itself be an articulatory suppression task. In line with these findings, another study showed that for right-handed subjects, tapping with a finger of the right hand is more effective at interfering with performance of a verbal memory task than is tapping with a finger of the left hand [@Friedman1988]. Although Friedman et al.'s findings are difficult to interpret, because task priority was manipulated and this may have led to conflict resolution, which might have been dealt with differentially according to the hand involved, they do suggest that a finger tapping task is not always the best control for articulatory suppression. This might explain the decrease of self-reported rumination observed in our own study, after the finger-tapping, and suggests that we might observe different results by asking participants to tap with the finger of their non-dominant hand. We think it is important to note for future studies that our results, together with those of @Saito1994 and @Friedman1988, suggest that finger-tapping could in fact interfere with inner speech. In other words, finger-tapping, with the dominant hand, is probably not an appropriate control condition when studying articulatory suppression.

As suggested previously, an alternative way to explain the absence of differences between the two motor conditions is to suppose that the effects of the articulatory suppression were weaker than we expected. The rhythmic mouthing task might have become too automatised to disrupt inner speech programming. This idea finds some support in the results of @Saito1997, who observed an effect of articulatory suppression on the phonological similarity effect in a memory task only when the articulatory suppression was *intermittent* (i.e., "ah, ah, ah...") but no effect when participants had to utter a continuous "ah--". This can be explained by considering that the intermittent articulatory suppression would impose a greater load on speech motor programming than the continuous articulatory suppression [@Saito1997, page 569]. In a similar vein, @Macken1995 found stronger effects of articulatory suppression when participants were asked to repeat a sequence of different letters than when they were asked to repeat a single letter. One way to examine this hypothesis with our own protocol would be to ask participants to make sequences of various mouth movements, rather than repeating a single movement.

In a broader perspective, relating to the original research question, we should mention two additional interpretations of our results. So far, we considered different ways to explain either how the finger-tapping task could interfere with rumination or how the articulatory suppression task might have failed to disrupt rumination. However, if we assume that our scales (especially the *RUM* outcome response and the *Verbality* scale) are reliable and that the articulatory suppression was efficient in its intended purpose, we are forced to admit that either i) rumination is not a type of inner speech that can be disrupted by peripheral muscle perturbation (i.e., it could be described as a more abstract form of inner speech) or that ii) inner speech, more broadly, does not depend on peripheral speech muscle activity. Although we think that these questions cannot be answered from our present results, we acknowledge that these two possibilities are compatible with our results.

In summary, the current research is one of the first behavioral studies exploring the association between verbal rumination and the speech motor system. While the observed data did not strictly corroborate our original hypotheses, we explored several explanations for the weak difference between articulatory suppression and the control task, and related our findings to previous works on the role of inner speech in verbal working memory. These results have important implications for future studies on articulatory suppression during inner speech or working memory tasks. More precisely, they highlight the need for further investigation of the most appropriate control task when studying the effects of articulatory suppression.

## Supplementary materials {#supp}

Pre-registered protocol, preprint, data, as well as reproducible code and figures are available at: [osf.io/3bh67](http://osf.io/3bh67).

A lot of useful packages have been used for the writing of this paper, among which the `papaja` and `knitr` packages for writing and formatting [@R-papaja;@R-knitr], the `ggplot2`, `ggforce`, `GGally`, `DiagrammeR`, and `plotly` packages for plotting [@R-ggplot2;@R-ggforce;@R-GGally;@R-DiagrammeR;@R-plotly], the `AICcmodavg`, and `Rmisc` packages for data analysis [@R-AICcmodavg;@R-Rmisc], as well as the `tidyverse` and `broom` packages for code writing and formatting [@R-broom;@R-tidyverse].

## Acknowledgements

This project was funded by the ANR project INNERSPEECH (grant number ANR-13-BSH2-0003-01). The first author of the manuscript is funded by a fellowship from Université Grenoble Alpes. We thank David Meary for his technical support in programming the eye-tracking experiment. We thank Rafael Laboissiere and Brice Beffara for their advice concerning data analysis.

## Appendix A. Eye-tracking control experiment

The purpose of this control experiment was to demonstrate that the two motor tasks used in the main experiment, namely, finger tapping and articulatory suppression (mouth movements) were equivalent in terms of task difficulty or general dual-task demand [@Emerson2003]. Participants performed a computer-based visual search task (i.e., finding a T among an array of Ls), adapted from the @Treisman1980 paradigm (see below for details).

### Sample

Twenty-four participants (Mean age = 19.46, SD = 1.18, Min-Max = 18-21, 21 females, 21 right-handed), drawn from the same population (i.e., undergraduate psychology students) as the main experiment took part in this eye-tracking pretest.

### Sample size

As we aimed to compare four conditions (i.e., visual search (VS) task alone, VS + finger tapping, VS + foot tapping and VS + mouth movements), we recruited 24 participants in order to have at least one participant per order in our random counter-balanced repeated measures design ($n = k!$ where $n$ is the number of possible orders of conditions for $k$ conditions, then $n =4 != 24$).

### Material

Experiment took place individually in a dark room. Participants had to seat in front of a 22 inches, Iyama Vision Master Pro 513-MA203DT CRT Monitor (resolution: 1024x768 pixels, refresh rate: 85 Hz) with a NVIDIA GeForce 9800 GTX+ graphic processor. A camera-based eye-tracker (EyeLink\textregistered\ 1000 from SR Research) with a sampling rate of 250 Hz and a minimum accuracy of 0.5° was used, in the pupil-corneal reflection tracking mode. Participants were positioned on a seat so as to keep distance from the camera to the forehead target between 50 and 60 cm. A five-point calibration was completed before presenting stimuli, at the beginning of each condition.

### Procedure

The target (i.e., the letter "T") was present at each trial, either on the right or on the left of the central vertical axis of the grid. The grid was an array of 6*6 items. Each stimulus was displayed until the participant response (maximum duration in case of no response: 5 seconds). Each grid of letters was preceded by a central fixation circle, that was displayed for 500ms after the participant moved his/her gaze towards it. In order to give their response ("left" or "right"), participants had to gaze towards a large filled gray circle, situated either on the left or on the right side of the grid. Each participant went through each condition, in a random order. A first general training session was proposed, at the beginning of the experiment, using ten items that were not used subsequently in the four conditions. Each condition was composed of 90 trials (45 left and 45 right), knowing that the first ten trials of each condition were considered as training trials and thus not included in analysis. All participants were filmed in order to ensure that they effectively performed the motor activity. 
Our measure of interest was the delay between the apparition of the grid and the participant's response (the time at which his/her gaze reached the response circle), below referred to as "response time" (RT).

### Data preprocessing

Raw data from EyeLink\textregistered\ includes gaze on screen spatial coordinates, pupil diameter and forehead target spatial coordinates, with its distance from the camera. For this experiment, since only RTs (in ms) of correct trials are interesting, invalid trials (when no response has been given) and wrong responses were removed from the analysis.

### Data analysis

Data were analysed using *Condition* (4 modalities) as a within-subject predictor and the natural logarithm of the RT as a dependent variable in a MLM, including a varying intercept for both *participant* and *item*. Comparisons of interest were computed using Helmert contrasts. Estimates and confidence intervals are reported for each comparison in the log scale.

### Results

```{r, results = "hide", warning = FALSE}
##################################################
# importing eyetracking data
###################################

eye_track_data <- read.csv("data/eyetracking_control.csv", header = TRUE, sep = ",")[, - 1]

#############################
# MLMs
#####################

cHelmert <- contr.helmert(4)
cHelmert2 <- cHelmert[c(4:1), 3:1]

mod1 <- lme4::lmer(log(RT) ~ 1 + (1|Participant) + (1|Item),
    REML = FALSE, eye_track_data)

mod2 <- lme4::lmer(log(RT) ~ 1 + Condition + (1|Participant) + (1|Item),
    REML = FALSE, eye_track_data, contrasts = list(Condition = cHelmert2) )

###############################
# summary table
#######################

parametersET <-
    mod2 %>%
    tidy(., effects = "fixed", conf.int = TRUE) %>%
    select(-statistic) %>%
    filter(!term == "(Intercept)") %>%
    mutate(
        term = c("$Control\\ vs\\ all$", "$Foot\\ vs\\ Finger + Mouth$", "$Finger\\ vs\\ Mouth$") ) %>%
    `colnames<-`(c("","$Est$","$SE$","$Lower$","$Upper$") )

#################################################
# preparing summary data for plot
#######################################

agg2 <-
    eye_track_data %>%
    group_by(Condition) %>%
    summarise(Lower = CI(RT)[3], Mean = CI(RT)[2], Upper = CI(RT)[1]) %>%
    ungroup() %>%
    data.frame
```

Results of the MLM are reported in Table \@ref(tab:paramET) and Figure \@ref(fig:eyetrack). Contrast analysis revealed a slight difference between the *Control* condition and the mean of the three other conditions (Est = `r parametersET[1,2]`, 95% CI = [`r parametersET[1,4]`, `r parametersET[1,5]`]) as well as a slight difference between the *Foot* condition and the mean of the *Finger* and the *Mouth* conditions (Est = `r parametersET[2,2]`, 95% CI = [`r parametersET[2,4]`, `r parametersET[2,5]`]) while no apparent differences between the *Mouth* and the *Finger* conditions (Est = `r parametersET[3,2]`, 95% CI = [`r parametersET[3,4]`, `r parametersET[3,5]`]).

```{r paramET, results = "asis"}
kable(
    parametersET,
    #placement = "H",
    align = c("l","c","c","c","c"),
    caption = "Coefficient estimates (Est), standard errors (SE), and 95\\% CIs (Lower, Upper) of each contrast.",
    #note = NULL,
    #small = TRUE,
    format.args = list(
        digits = c(3, 3, 3, 3),
        margin = 2,
        decimal.mark = ".", big.mark = ""),
    escape = FALSE
    )
```

```{r eyetrack, fig.pos = "H", fig.width = 8, fig.height = 6, fig.cap = "Mean RTs by Condition along with 95% CIs and violin plots (the horizontal line represents the median of the conditional distribution). Grey dots represent mean RTs by participant."}
detach(package:Rmisc)
detach(package:plyr)

library(dplyr)

eye_track_data %>%
    # aggregating by item
    dplyr::group_by(Condition, Participant) %>%
    summarise(
        Lower = Rmisc::CI(RT)[3],
        Mean = Rmisc::CI(RT)[2],
        Upper = Rmisc::CI(RT)[1] ) %>%
    ungroup() %>%
    # plotting
    ggplot(aes(x = Condition, y = Mean) ) +
    # violin plots
    geom_violin(
        scale = "count", alpha = 0.2,
        color = "grey", fill = "grey",
        draw_quantiles = 0.5,
        show.legend = FALSE) +
    # plotting individual data points
    geom_dotplot(
        stackdir = "center",
        binaxis = "y",
        binwidth = 50,
        dotsize = 0.3, alpha = 0.2) +
    # plotting means
    stat_summary(
        fun.y = "mean", geom = "point",
        shape = 16, size = 5,
        show.legend = TRUE) +
    # plotting confidence intervals
    stat_summary(
        fun.data = mean_cl_normal,
        geom = "errorbar", size = 1, width = 0,
        fun.args = list(mult = 1.96),
        show.legend = FALSE) +
    labs(title = "", x = "", y = "Response time (in ms)") +
    theme_bw(base_size = 14)
```

### Discussion

This control experiment shows that there is no apparent difference (or a negligible one) in terms of attentional demand between the two motor tasks used in the main experiment (i.e., finger-tapping and mouthing), although performing a dual motor task (of any type) does seem costly, because of the observed difference between the control condition and the mean of the three others conditions. These results are in line with the results obtained by @Cefidekhanie2014 in their control experiment.
