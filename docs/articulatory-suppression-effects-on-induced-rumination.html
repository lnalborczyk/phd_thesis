<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Psychophysiological characteristics of verbal rumination</title>
  <meta name="description" content="Blah blah blah…">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Psychophysiological characteristics of verbal rumination" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Blah blah blah…" />
  <meta name="github-repo" content="lnalborczyk/phd_thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Psychophysiological characteristics of verbal rumination" />
  
  <meta name="twitter:description" content="Blah blah blah…" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="zygoto-experiment.html">
<link rel="next" href="tms-study.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="part"><span><b>I Theoretical framework</b></span></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Overt and imagined actions</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#motor-imagery"><i class="fa fa-check"></i><b>1.1</b> Motor imagery</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro.html"><a href="intro.html#simulation-theories"><i class="fa fa-check"></i><b>1.1.1</b> Simulation theories</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro.html"><a href="intro.html#emulation-theories"><i class="fa fa-check"></i><b>1.1.2</b> Emulation theories</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro.html"><a href="intro.html#action-representation-and-internal-models"><i class="fa fa-check"></i><b>1.1.3</b> Action representation and internal models</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#inner-speech"><i class="fa fa-check"></i><b>1.2</b> Inner speech</a><ul>
<li class="chapter" data-level="1.2.1" data-path="intro.html"><a href="intro.html#mvtv-cohen-1986"><i class="fa fa-check"></i><b>1.2.1</b> MVTV Cohen (1986)</a></li>
<li class="chapter" data-level="1.2.2" data-path="intro.html"><a href="intro.html#predictive-models"><i class="fa fa-check"></i><b>1.2.2</b> Predictive models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rumination-as-simulated-speech.html"><a href="rumination-as-simulated-speech.html"><i class="fa fa-check"></i><b>2</b> Rumination as simulated speech</a></li>
<li class="chapter" data-level="3" data-path="electromyographic-correlates-of-speech-production.html"><a href="electromyographic-correlates-of-speech-production.html"><i class="fa fa-check"></i><b>3</b> Electromyographic correlates of speech production</a><ul>
<li class="chapter" data-level="3.1" data-path="electromyographic-correlates-of-speech-production.html"><a href="electromyographic-correlates-of-speech-production.html#speech-production-mechanisms"><i class="fa fa-check"></i><b>3.1</b> Speech production mechanisms</a></li>
<li class="chapter" data-level="3.2" data-path="electromyographic-correlates-of-speech-production.html"><a href="electromyographic-correlates-of-speech-production.html#speech-production-muscles"><i class="fa fa-check"></i><b>3.2</b> Speech production muscles</a></li>
<li class="chapter" data-level="3.3" data-path="electromyographic-correlates-of-speech-production.html"><a href="electromyographic-correlates-of-speech-production.html#muscular-physiology"><i class="fa fa-check"></i><b>3.3</b> Muscular physiology</a></li>
<li class="chapter" data-level="3.4" data-path="electromyographic-correlates-of-speech-production.html"><a href="electromyographic-correlates-of-speech-production.html#emg-signal"><i class="fa fa-check"></i><b>3.4</b> EMG signal</a><ul>
<li class="chapter" data-level="3.4.1" data-path="electromyographic-correlates-of-speech-production.html"><a href="electromyographic-correlates-of-speech-production.html#emg-signal-measures"><i class="fa fa-check"></i><b>3.4.1</b> EMG signal measures</a></li>
<li class="chapter" data-level="3.4.2" data-path="electromyographic-correlates-of-speech-production.html"><a href="electromyographic-correlates-of-speech-production.html#motor-unit-action-potential"><i class="fa fa-check"></i><b>3.4.2</b> Motor Unit Action Potential</a></li>
<li class="chapter" data-level="3.4.3" data-path="electromyographic-correlates-of-speech-production.html"><a href="electromyographic-correlates-of-speech-production.html#surface-emg"><i class="fa fa-check"></i><b>3.4.3</b> Surface EMG</a></li>
<li class="chapter" data-level="3.4.4" data-path="electromyographic-correlates-of-speech-production.html"><a href="electromyographic-correlates-of-speech-production.html#basic-signal-processing"><i class="fa fa-check"></i><b>3.4.4</b> Basic signal processing</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Experimental part</b></span></li>
<li class="chapter" data-level="4" data-path="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html"><a href="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html"><i class="fa fa-check"></i><b>4</b> Orofacial electromyographic correlates of induced verbal rumination</a><ul>
<li class="chapter" data-level="4.1" data-path="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html"><a href="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html"><a href="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html#methods"><i class="fa fa-check"></i><b>4.2</b> Methods</a><ul>
<li class="chapter" data-level="4.2.1" data-path="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html"><a href="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html#participants"><i class="fa fa-check"></i><b>4.2.1</b> Participants</a></li>
<li class="chapter" data-level="4.2.2" data-path="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html"><a href="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html#material"><i class="fa fa-check"></i><b>4.2.2</b> Material</a></li>
<li class="chapter" data-level="4.2.3" data-path="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html"><a href="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html#procedure"><i class="fa fa-check"></i><b>4.2.3</b> Procedure</a></li>
<li class="chapter" data-level="4.2.4" data-path="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html"><a href="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html#data-processing-and-analysis"><i class="fa fa-check"></i><b>4.2.4</b> Data processing and analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html"><a href="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html#results"><i class="fa fa-check"></i><b>4.3</b> Results</a><ul>
<li class="chapter" data-level="4.3.1" data-path="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html"><a href="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html#experiment-1-rumination-induction-1"><i class="fa fa-check"></i><b>4.3.1</b> Experiment 1: rumination induction</a></li>
<li class="chapter" data-level="4.3.2" data-path="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html"><a href="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html#experiment-2-rumination-reduction-by-relaxation-1"><i class="fa fa-check"></i><b>4.3.2</b> Experiment 2: rumination reduction by relaxation</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html"><a href="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html#discussion"><i class="fa fa-check"></i><b>4.4</b> Discussion</a><ul>
<li class="chapter" data-level="4.4.1" data-path="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html"><a href="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html#experiment-1"><i class="fa fa-check"></i><b>4.4.1</b> Experiment 1</a></li>
<li class="chapter" data-level="4.4.2" data-path="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html"><a href="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html#experiment-2"><i class="fa fa-check"></i><b>4.4.2</b> Experiment 2</a></li>
<li class="chapter" data-level="4.4.3" data-path="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html"><a href="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html#general-discussion"><i class="fa fa-check"></i><b>4.4.3</b> General discussion</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html"><a href="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html#acknowledgements"><i class="fa fa-check"></i><b>4.5</b> Acknowledgements</a></li>
<li class="chapter" data-level="4.6" data-path="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html"><a href="orofacial-electromyographic-correlates-of-induced-verbal-rumination.html#supplementary-data"><i class="fa fa-check"></i><b>4.6</b> Supplementary data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="dissociating-facial-electromyographic-correlates-of-visual-and-verbal-induced-rumination.html"><a href="dissociating-facial-electromyographic-correlates-of-visual-and-verbal-induced-rumination.html"><i class="fa fa-check"></i><b>5</b> Dissociating facial electromyographic correlates of visual and verbal induced rumination</a></li>
<li class="chapter" data-level="6" data-path="zygoto-experiment.html"><a href="zygoto-experiment.html"><i class="fa fa-check"></i><b>6</b> Zygoto experiment</a></li>
<li class="chapter" data-level="7" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html"><i class="fa fa-check"></i><b>7</b> Articulatory suppression effects on induced rumination</a><ul>
<li class="chapter" data-level="7.1" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a><ul>
<li class="chapter" data-level="7.1.1" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#multisensory-and-motor-components-of-inner-speech"><i class="fa fa-check"></i><b>7.1.1</b> Multisensory and motor components of inner speech</a></li>
<li class="chapter" data-level="7.1.2" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#rumination"><i class="fa fa-check"></i><b>7.1.2</b> Rumination</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#methods-1"><i class="fa fa-check"></i><b>7.2</b> Methods</a><ul>
<li class="chapter" data-level="7.2.1" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#sample"><i class="fa fa-check"></i><b>7.2.1</b> Sample</a></li>
<li class="chapter" data-level="7.2.2" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#material-1"><i class="fa fa-check"></i><b>7.2.2</b> Material</a></li>
<li class="chapter" data-level="7.2.3" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#procedure-1"><i class="fa fa-check"></i><b>7.2.3</b> Procedure</a></li>
<li class="chapter" data-level="7.2.4" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#data-analysis"><i class="fa fa-check"></i><b>7.2.4</b> Data analysis</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#results-1"><i class="fa fa-check"></i><b>7.3</b> Results</a><ul>
<li class="chapter" data-level="7.3.1" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#correlation-matrix-between-main-predictors-and-control-variables"><i class="fa fa-check"></i><b>7.3.1</b> Correlation matrix between main predictors and control variables</a></li>
<li class="chapter" data-level="7.3.2" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#rumination-induction-2"><i class="fa fa-check"></i><b>7.3.2</b> Rumination induction</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#articulatory-suppression-effects-on-induced-rumination-1"><i class="fa fa-check"></i><b>7.4</b> Articulatory suppression effects on induced rumination</a></li>
<li class="chapter" data-level="7.5" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#discussion-1"><i class="fa fa-check"></i><b>7.5</b> Discussion</a><ul>
<li class="chapter" data-level="7.5.1" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#rumination-induction-3"><i class="fa fa-check"></i><b>7.5.1</b> Rumination induction</a></li>
<li class="chapter" data-level="7.5.2" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#articulatory-suppression-effects-1"><i class="fa fa-check"></i><b>7.5.2</b> Articulatory suppression effects</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#supp"><i class="fa fa-check"></i><b>7.6</b> Supplementary materials</a></li>
<li class="chapter" data-level="7.7" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#acknowledgements-1"><i class="fa fa-check"></i><b>7.7</b> Acknowledgements</a></li>
<li class="chapter" data-level="7.8" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#appendix-a.-eye-tracking-control-experiment"><i class="fa fa-check"></i><b>7.8</b> Appendix A. Eye-tracking control experiment</a><ul>
<li class="chapter" data-level="7.8.1" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#sample-1"><i class="fa fa-check"></i><b>7.8.1</b> Sample</a></li>
<li class="chapter" data-level="7.8.2" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#sample-size"><i class="fa fa-check"></i><b>7.8.2</b> Sample size</a></li>
<li class="chapter" data-level="7.8.3" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#material-2"><i class="fa fa-check"></i><b>7.8.3</b> Material</a></li>
<li class="chapter" data-level="7.8.4" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#procedure-2"><i class="fa fa-check"></i><b>7.8.4</b> Procedure</a></li>
<li class="chapter" data-level="7.8.5" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#data-preprocessing"><i class="fa fa-check"></i><b>7.8.5</b> Data preprocessing</a></li>
<li class="chapter" data-level="7.8.6" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#data-analysis-1"><i class="fa fa-check"></i><b>7.8.6</b> Data analysis</a></li>
<li class="chapter" data-level="7.8.7" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#results-2"><i class="fa fa-check"></i><b>7.8.7</b> Results</a></li>
<li class="chapter" data-level="7.8.8" data-path="articulatory-suppression-effects-on-induced-rumination.html"><a href="articulatory-suppression-effects-on-induced-rumination.html#discussion-2"><i class="fa fa-check"></i><b>7.8.8</b> Discussion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="tms-study.html"><a href="tms-study.html"><i class="fa fa-check"></i><b>8</b> TMS study</a></li>
<li class="part"><span><b>III General discussion and conclusions</b></span></li>
<li class="chapter" data-level="9" data-path="general-discussion-1.html"><a href="general-discussion-1.html"><i class="fa fa-check"></i><b>9</b> General discussion</a></li>
<li class="chapter" data-level="10" data-path="conclusions.html"><a href="conclusions.html"><i class="fa fa-check"></i><b>10</b> Conclusions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank"> Powered by bookdown </a></li>
<li><a href="http://www.barelysignificant.com" target="blank"> Ladislas Nalborczyk </a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Psychophysiological characteristics of verbal rumination</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="articulatory-suppression-effects-on-induced-rumination" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Articulatory suppression effects on induced rumination</h1>
<p>Summary of the research…<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p>
<div id="introduction-1" class="section level2">
<h2><span class="header-section-number">7.1</span> Introduction</h2>
<p>A large part of our inner experience involves verbal content, with internal monologues and conversations. Inner speech is considered as a major component of conscious experience and cognition <span class="citation">(<span class="citeproc-not-found" data-reference-id="Hubbard2010"><strong>???</strong></span>, <span class="citeproc-no-output"><strong>???</strong></span>, <span class="citeproc-no-output"><strong>???</strong></span>)</span>. An important issue related to inner speech concerns its format and nature and whether it is better described as a mere evocation of abstract amodal verbal representations or as a concrete motor simulation of actual speech production. In the first case, inner speech is seen as divorced from bodily experience, and includes, at most, faded auditory representations. In the second case, inner speech is considered as a physical process that unfolds over time, leading to an enactive re-creation of auditory a well as articulatory percepts. The latter hypothesis is interesting in the context of persistent negative and maladaptive forms of inner speech, such as rumination. If this hypothesis is correct, we could expect rumination –as a particular type of inner speech– to be disrupted by concurrent involvement of the speech muscles.</p>
<div id="multisensory-and-motor-components-of-inner-speech" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Multisensory and motor components of inner speech</h3>
<p>Introspective explorations of the characteristics of inner speech have led to different views on the relative importance of its auditory and articulatory components, and on the involvement of motor processes. For <span class="citation">(<span class="citeproc-not-found" data-reference-id="Stricker1880"><strong>???</strong></span>)</span>, “speech representations are motor representations”, while <span class="citation">(<span class="citeproc-not-found" data-reference-id="Egger1881"><strong>???</strong></span>)</span> believed that auditory representations are dominant in inner speech and that motor representations are not always present. However, as noted by <span class="citation">(<span class="citeproc-not-found" data-reference-id="Ballet1886"><strong>???</strong></span>)</span>, these contradictory hypotheses might stem from an over-generalization of their own introspective findings, the former discovering motor feelings and the latter auditory images <span class="citation">(<span class="citeproc-not-found" data-reference-id="Ballet1886"><strong>???</strong></span>)</span>. In the same vein, <span class="citation">(<span class="citeproc-not-found" data-reference-id="Paulhan1886"><strong>???</strong></span>)</span> claimed that inner speech involves both auditory and motor images, defining motor images as the sensations in the speech organs (larynx, tongue, lips) that sometimes accompany inner speech. Therefore, Paulhan’s notion of <em>motor images</em> are in fact related to somatosensory representations rather than to the involvement of actual speech movements (as claimed by Stricker). It remains, however, that a distinction was introduced by 19th century authors between sensory and motor phenomena in inner speech, with sensory phenomena including auditory as well as articulatory (or somatosensory) percepts. The intuitive distinction between auditory and motor phenomena is referred to in contemporary research by the terms of <em>inner ear</em> and <em>inner voice</em>, in line with Baddeley’s classic model of working memory <span class="citation">(e.g., <span class="citeproc-not-found" data-reference-id="Baddeley1974"><strong>???</strong></span>, see also <span class="citeproc-no-output"><strong>???</strong></span>)</span>. Baddeley’s model relies on a partnership between an <em>inner ear</em> (i.e., storage) and an <em>inner voice</em> <span class="citation">(i.e., subvocal rehearsal; see <span class="citeproc-not-found" data-reference-id="Smith1995"><strong>???</strong></span>)</span>.</p>
<p>Empirical arguments supporting the crucial role of the inner voice in verbal memory (subvocal rehearsal) and auditory imagery can be found in studies using articulatory suppression, in which the <em>action</em> component (i.e., the <em>inner voice</em>) of inner speech is disrupted. Articulatory suppression usually refers to a task which requires participants to utter speech sounds (or to produce speech gestures without sound), so that this activity disrupts ongoing speech production processes. Articulatory suppression can be produced with different degrees of vocalisation, going from overt uttering of irrelevant words, to whispering, mouthing (i.e., silent articulation), and simple clamping of the speech articulators. Many verbal working memory studies have shown that articulatory suppression impairs recall performance <span class="citation">(e.g., <span class="citeproc-not-found" data-reference-id="Baddeley1984"><strong>???</strong></span>)</span>.</p>
<p>In a study aiming at investigating the role of <em>covert enactment</em> in auditory imagery, <span class="citation">(<span class="citeproc-not-found" data-reference-id="Reisberg1989"><strong>???</strong></span>)</span> observed that the verbal transformation effect <span class="citation">(VTE, <span class="citeproc-not-found" data-reference-id="Warren1958"><strong>???</strong></span>)</span>, namely the alteration of speech percepts when certain speech sounds are uttered in a repetitive way, also occurred during inner speech (although the VTE was smaller than during overt speech), but was suppressed by concurrent articulation (e.g., chewing) or clamping the articulators. The fact that the VTE was observed during inner speech and that it was reduced by concurrent chewing, even in inner speech, speaks in favour of the view of inner speech as an enacted simulation of overt speech.</p>
<p>Another piece of evidence for the effect of articulatory suppression on inner speech comes from a recent study by <span class="citation">(<span class="citeproc-not-found" data-reference-id="Topolinski2009"><strong>???</strong></span>)</span> on the mere exposure effect, namely the fact that repeated exposure to a stimulus influences the evaluation of this stimulus in a positive way <span class="citation">(<span class="citeproc-not-found" data-reference-id="Zajonc1968"><strong>???</strong></span>)</span>. Topolinski and Strack’s study showed that the mere exposure effect for visually presented verbal material could be completely suppressed by blocking subvocal rehearsal (i.e., inner speech) when asking participants to chew a gum. The effect was preserved, however, when participants kneaded a soft ball with their hand <span class="citation">(<span class="citeproc-not-found" data-reference-id="Topolinski2009"><strong>???</strong></span>)</span>. This finding suggests that blocking speech motor simulation interfered with the inner rehearsal of the visually presented verbal stimuli, thereby destroying the positive exposure effect. It provides additional experimental support to the view that inner speech involves a motor component.</p>
<p>The occurrence of motor simulation during inner speech is further backed by several studies using physiological measures to evaluate inner speech production properties. Using electrodes inserted in the tongue tip or lips of five participants, <span class="citation">(<span class="citeproc-not-found" data-reference-id="Jacobson1931"><strong>???</strong></span>)</span> was able to detect electromyographic (EMG) activity during several tasks requiring inner speech. Similarly, <span class="citation">(<span class="citeproc-not-found" data-reference-id="Sokolov1972"><strong>???</strong></span>)</span> recorded intense lip and tongue muscle activation when participants had to perform complex tasks that necessitated substantial inner speech production (e.g., problem solving). Another study using surface electromyography (sEMG) demonstrated an increase in activity of the lip muscles during silent recitation tasks compared to rest, but no increase during the non-linguistic visualisation task <span class="citation">(<span class="citeproc-not-found" data-reference-id="Livesay1996"><strong>???</strong></span>)</span>. An increase in the lip and forehead muscular activity has also been observed during induced rumination <span class="citation">(<span class="citeproc-not-found" data-reference-id="Nalborczyk2017"><strong>???</strong></span>)</span>. Furthermore, this last study also suggested that speech-related muscle relaxation was slightly more efficient in reducing subjective levels of rumination than non speech-related muscle relaxation, suggesting that relaxing or inhibiting the speech muscles could disrupt rumination.</p>
</div>
<div id="rumination" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Rumination</h3>
<p>Rumination is a “class of conscious thoughts that revolve around a common instrumental theme and that recur in the absence of immediate environmental demands requiring the thoughts” <span class="citation">(<span class="citeproc-not-found" data-reference-id="Martin1996"><strong>???</strong></span>)</span>. Despite the fact that depressed patients report positive metacognitive beliefs about ruminating, which is often seen as a coping strategy in order to regulate mood <span class="citation">(e.g., <span class="citeproc-not-found" data-reference-id="Papageorgiou2001"><strong>???</strong></span>)</span>, rumination is known to significantly worsen mood <span class="citation">(e.g., <span class="citeproc-not-found" data-reference-id="Moberly2008"><strong>???</strong></span>, <span class="citeproc-no-output"><strong>???</strong></span>)</span>, impair cognitive flexibility <span class="citation">(e.g., <span class="citeproc-not-found" data-reference-id="Davis2000"><strong>???</strong></span>, <span class="citeproc-no-output"><strong>???</strong></span>)</span>, and to lead toward pronounced social exclusion and more interpersonnal distress <span class="citation">(<span class="citeproc-not-found" data-reference-id="Lam2003"><strong>???</strong></span>)</span>. Although partly visual, rumination is a predominantly verbal process <span class="citation">(<span class="citeproc-not-found" data-reference-id="Goldwin2012"><strong>???</strong></span>, <span class="citeproc-no-output"><strong>???</strong></span>)</span> and can be considered as a maladaptive type of inner speech.</p>
<p>In a study on worry, another form of repetitive negative thinking, <span class="citation">(<span class="citeproc-not-found" data-reference-id="Rapee1993"><strong>???</strong></span>)</span> observed a <em>tendency</em> for articulatory suppression, but not for visuo-spatial tasks, to produce some interference with worrying. He concluded that worry involves the phonological aspect of the central executive of working memory. We further add that, since repeating a word seems to reduce the ability to worry, this study suggests that articulatory aspects are at play during worry.</p>
<p>In this context, the question we addressed in this study is whether verbal rumination consists of purely abstract verbal representations or whether it is better described as a motor simulation of speech production, engaging the speech apparatus. If the latter hypothesis is correct, rumination experienced in verbal form (in contrast to a non-verbal form) should be disrupted by mouthing (i.e., silent articulation), and should not be disrupted by a control task that does not involve speech muscles (e.g., finger-tapping). Specifically, we thus sought to test the hypotheses that rumination could be disrupted by articulatory suppression (but not by finger-tapping), and that this disruption would be more pronounced when rumination is experienced in a verbal form than in a non-verbal form.</p>
</div>
</div>
<div id="methods-1" class="section level2">
<h2><span class="header-section-number">7.2</span> Methods</h2>
<p>In the <em>Methods</em> and <em>Data analysis</em> sections, we report how we determined our sample size, all data exclusions, all manipulations, and all measures in the study <span class="citation">(<span class="citeproc-not-found" data-reference-id="Simmons2012"><strong>???</strong></span>)</span>. A pre-registered version of our protocol can be found on OSF: <a href="https://osf.io/3bh67/" class="uri">https://osf.io/3bh67/</a>.</p>
<div id="sample" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Sample</h3>
<p>We originally planned for 128 participants to take part in the study. This sample size was set on the basis of results obtained by <span class="citation">(<span class="citeproc-not-found" data-reference-id="Topolinski2009"><strong>???</strong></span>)</span>, who observed an effect size around <span class="math inline">\(\eta_{p}^{2}=.06\)</span>. We expected a similar effect size for the current rumination disruption, since rumination can be conceived of as a subtype of inner speech<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a>.</p>
<p>As we anticipated drop-out of participants due to our inclusion criteria (see below), a total of 184 undergraduate students in psychology from the Université Grenoble Alpes took part in this experiment, in exchange for course credits. They were recruited via mailing list, online student groups, and posters. Each participant provided a written consent and this study was approved by the local ethics committee (CERNI N° 2016-05-31-9). To be eligible, participants had to be between 18 and 35 years of age, with no history of motor, neurological, psychiatric, or speech-development disorders. All participants spoke French as their mother tongue. After each participant gave their written consent, they completed the Center for Epidemiologic Studies - Depression scale <span class="citation">(CES-D; <span class="citeproc-not-found" data-reference-id="Radloff1977a"><strong>???</strong></span>)</span>. The CES-D is a 12-item questionnaire, validated in French <span class="citation">(<span class="citeproc-not-found" data-reference-id="Morin2011"><strong>???</strong></span>)</span>, aiming to assess the level of depressive symptoms in a subclinical population. Participants exceeding the threshold of clinical depressive symptoms <span class="citation">(i.e., &gt;23 for females and &gt;17 for males; <span class="citeproc-not-found" data-reference-id="Radloff1977a"><strong>???</strong></span>)</span> were not included in the study for ethical reasons (N = 26).</p>
<p>To investigate articulatory suppression effects in the context of rumination, a successful induction of rumination is a prerequisite. Therefore, analyses were only conducted on participants who showed an effect of the rumination induction (i.e., strictly speaking, participants who reported more rumination after the induction than before). We thus discarded participants who did not show any increase in rumination level (N = 52, 32.91% of total sample). The final sample comprised 106 participants (Mean age = 20.3018868, SD = 2.5728064, Min-Max = 18-31, 96 females).</p>
</div>
<div id="material-1" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Material</h3>
<p>The experiment was programmed with OpenSesame software <span class="citation">(<span class="citeproc-not-found" data-reference-id="Mathot2012"><strong>???</strong></span>)</span> and stimuli were displayed on a DELL latitude E6500 computer screen.</p>
<div id="questionaires" class="section level4">
<h4><span class="header-section-number">7.2.2.1</span> Questionaires</h4>
<p>To control for confounding variables likely to be related to the intensity of the induction procedure, we administered the French version of the Positive and Negative Affect Schedule <span class="citation">(PANAS; <span class="citeproc-not-found" data-reference-id="Watson1988"><strong>???</strong></span>)</span>, adapted to French by <span class="citation">(<span class="citeproc-not-found" data-reference-id="Gaudreau2006"><strong>???</strong></span>)</span>. This questionnaire includes 20 items, from which we can compute an overall index of both positive (by summing the scores on 10 positive items, thereafter <em>PANASpos</em>) and negative affect (<em>PANASneg</em>) at baseline. This questionnaire was administered at baseline. In order to evaluate trait rumination, at the end of the experiment participants completed the short version of the Ruminative Response Scale <span class="citation">(RRS-R, <span class="citeproc-not-found" data-reference-id="Treynor2003"><strong>???</strong></span>)</span>, validated in French (Douilliez, Guimpel, Baeyens, &amp; Philippot, <em>in preparation</em>). From this questionnaire, scores on two dimensions were analysed (<em>RRSbrooding</em> and <em>RRSreflection</em>).</p>
</div>
<div id="measures" class="section level4">
<h4><span class="header-section-number">7.2.2.2</span> Measures</h4>
<p>Measures of state rumination were recorded using a Visual Analogue Scale (VAS) previously used in <span class="citation">(<span class="citeproc-not-found" data-reference-id="Nalborczyk2017"><strong>???</strong></span>)</span>. This scale measured the degree of agreement with the sentence “At this moment, I am brooding on negative things” (translated from French), on a continuum between “Not at all” and “A lot” (afterwards coded between 0 and 100). This scale is subsequently referred to as the <em>RUM</em> scale. It was used three times in the experiment, at baseline (after training but before the experiment started), after rumination induction, and after a motor task.</p>
<p>Additionally, participants answered questions about the modality of the thoughts that occurred while performing the motor task. This last questionnaire consisted of one question evaluating the occurrence frequency of different modalities of inner thoughts (e.g., visual imagery, verbal thoughts, music). Then, a verbal/non-verbal ratio (i.e., the score on the verbal item divided by the mean of the score on the non-verbal items) was computed, hereafter referred to as the <em>Verbality</em> continuous predictor (this scale is available online: <a href="https://osf.io/3bh67/" class="uri">https://osf.io/3bh67/</a>).</p>
</div>
<div id="tasks" class="section level4">
<h4><span class="header-section-number">7.2.2.3</span> Tasks</h4>
<p>In the first part of the experiment, ruminative thoughts were induced using a classical induction procedure. Then a motor task was executed. Participants were randomly allocated to one of two conditions. In the <em>Mouthing</em> condition, the task consisted of repetitively making mouth opening-closing movements at a comfortable pace. This condition was selected as it is commonly used in articulatory suppression studies <span class="citation">(e.g., <span class="citeproc-not-found" data-reference-id="Baddeley1984"><strong>???</strong></span>)</span>. As a control, a finger-tapping condition was used (the <em>Tapping</em> condition), that consisted of tapping on the desk with the index finger of the dominant hand at a comfortable pace.</p>
<p>Although finger-tapping tasks are generally considered as good control conditions when using speech motor tasks, since they are comparable in terms of general attentional demands, it may be that orofacial gestures are intrinsically more complex than manual gestures <span class="citation">(i.e., more costly, <span class="citeproc-not-found" data-reference-id="Emerson2003"><strong>???</strong></span>)</span>. To discard the possibility that orofacial gestures (related to the <em>Mouthing</em> condition) would be cognitively more demanding than manual ones (related to the <em>Tapping</em> condition), we designed a pretest experiment in order to compare the two interference motor tasks used in the main experiment. Results of this control experiment showed no difference on reaction times during a visual search task between the two interference tasks (i.e., mouthing and finger-tapping). Full details are provided in Appendix A.</p>
</div>
</div>
<div id="procedure-1" class="section level3">
<h3><span class="header-section-number">7.2.3</span> Procedure</h3>
<p>The experiment took place individually in a quiet and dimmed room. The total duration of the session ranged between 35min and 40min. Before starting the experiment, participants were asked to perform the motor task during 1 min, while following a dot moving at a random pace on the screen in front of them. This task was designed to train the participants to perform the motor task adequately. Following this training and after describing the experiment, the experimenter left the room and each participant had to fill-in a baseline questionnaire (adaptation of PANAS, see above) presented on the computer screen. Baseline state rumination was then evaluated using the <em>RUM</em> scale. The whole experiment was video-monitored using a Sony HDR-CX240E video camera, in order to check that the participants effectively completed the task.</p>
<div id="rumination-induction" class="section level4">
<h4><span class="header-section-number">7.2.3.1</span> Rumination induction</h4>
<p>Rumination induction consisted of two steps. The first step consisted of inducing a negative mood in order to enhance the effects of the subsequent rumination induction. Participants were asked to recall a significant personal failure experienced in the past five years. Then, participants were invited to evaluate the extent to which this memory was “intense for them” on a VAS between “Not at all” and “A lot”, afterwards coded between 0 and 100, and referred to as <em>Vividness</em>.</p>
<p>The second step consisted of the rumination induction proper. We used a French translation of the <span class="citation">(<span class="citeproc-not-found" data-reference-id="Nolen-hoeksema1993"><strong>???</strong></span>)</span> rumination induction procedure. Participants had to read a list of 44 sentences related to the meaning, the causes and the consequences of their current affective or physiological state. Each phrase was presented on a computer screen for 10 seconds and the total duration of this step was 7 minutes and 20 seconds. State rumination was then evaluated again using the same VAS as the one used at baseline (<em>RUM</em>).</p>
</div>
<div id="proc_supp" class="section level4">
<h4><span class="header-section-number">7.2.3.2</span> Motor task</h4>
<p>After the rumination induction, participants were asked to continue to think about “the meaning, causes, and consequences” of their feelings while either repetitively making mouth movements (for participants allocated in the “Mouthing” condition) or finger-tapping with the dominant hand for five minutes (for participants allocated in the “Tapping” condition). Afterwards, state rumination was again evaluated using the <em>RUM</em> scale.</p>
<p>In order to evaluate trait rumination, participants completed the short version of the RRS (see above). Then were filled in the questionnaire on the modality of the thoughts that occurred while performing the motor task (see above). Figure <a href="articulatory-suppression-effects-on-induced-rumination.html#fig:diagram">7.1</a> summarises the full procedure.</p>
<div class="figure" style="text-align: center"><span id="fig:diagram"></span>
<img src="06-chap6_files/figure-html/diagram-1.pdf" alt="Timeline of the experiment, from top to bottom." width="576" />
<p class="caption">
Figure 7.1: Timeline of the experiment, from top to bottom.
</p>
</div>
</div>
</div>
<div id="data-analysis" class="section level3">
<h3><span class="header-section-number">7.2.4</span> Data analysis</h3>
<p>Statistical analyses were conducted using R version 3.4.3 <span class="citation">(R Core Team, <a href="#ref-R-base">2018</a>)</span>, and are reported with the <code>papaja</code> <span class="citation">(Aust &amp; Barth, <a href="#ref-R-papaja">2018</a>)</span> and <code>knitr</code> <span class="citation">(Xie, <a href="#ref-R-knitr">2018</a>)</span> packages.</p>
<div id="rumination-induction-1" class="section level4">
<h4><span class="header-section-number">7.2.4.1</span> Rumination induction</h4>
<p>We centered and standardised each predictor in order to facilitate the interpretation of parameters. Data were then analysed using <em>Induction</em> (2 modalities, before and after induction, contrast-coded) as a within-subject categorical predictor and <em>RUM</em> as a dependent variable in a multilevel linear model (MLM). Data were fitted using the <code>lmer</code> function, within the <code>lme4</code> package <span class="citation">(<span class="citeproc-not-found" data-reference-id="lme4"><strong>???</strong></span>)</span>. This model was compared with more complex models including effects of control variables, such as baseline affect state (<em>PANAS</em> scores) or the vividness of the memory chosen during the induction (<em>Vividness</em> score). Models were compared using the corrected Akaike Information Criterion (AICc) and evidence ratios <span class="citation">(<span class="citeproc-not-found" data-reference-id="Burnham2002"><strong>???</strong></span>, <span class="citeproc-no-output"><strong>???</strong></span>, <span class="citeproc-no-output"><strong>???</strong></span>)</span>. AICc provides a relative measure of predictive accuracy of the models (the AIC is an approximation of the out-of-sample deviance of a model) and balances underfitting and overfitting by sanctioning models for their number of parameters. We computed the difference between the best (lower) and other AICcs with <span class="math inline">\(\Delta_{AICc}=AICc_{i}-AICc_{min}\)</span> and then expressed the weight of a model as:</p>
<p><span class="math display">\[w_{i}=\dfrac{exp(-\Delta_{i}/2)}{\sum_{r=1}^{R}exp(-\Delta_{r}/2)}\]</span></p>
<p>From there, we computed evidence ratios (ERs) as the ratios of weights: <span class="math inline">\(ER_{ij} = \dfrac{w_{i}}{w_{j}}\)</span>, where <span class="math inline">\(w_{i}\)</span> and <span class="math inline">\(w_{j}\)</span> are the Akaike weights of models <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, respectively. These weights can be interpreted as the probability of the model being the best model in terms of out-of-sample prediction <span class="citation">(<span class="citeproc-not-found" data-reference-id="Burnham2002"><strong>???</strong></span>)</span>. Instead of reporting null-hypothesis tests for our MLMs, we report 95% confidence intervals for the constant effects estimates<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a>.</p>
<p>Whereas the use of AICc is appropriate for model comparison and selection, it tells us nothing about the absolute fit of the model. To estimate this fit, we computed two types of <span class="math inline">\(R^2\)</span> for MLMs using the <code>MuMIn</code> package <span class="citation">(<span class="citeproc-not-found" data-reference-id="MuMIn"><strong>???</strong></span>)</span>. The first, called the marginal <span class="math inline">\(R^2\)</span> (<span class="math inline">\(R^2_{marg.}\)</span>), estimates the proportion of variance accounted for by the constant effects, whereas the second, called the conditional <span class="math inline">\(R^2\)</span> (<span class="math inline">\(R^2_{cond.}\)</span>), estimates the proportion of variance accounted for by the constant and the varying effects taken together <span class="citation">(<span class="citeproc-not-found" data-reference-id="Getz2015"><strong>???</strong></span>, <span class="citeproc-no-output"><strong>???</strong></span>, <span class="citeproc-no-output"><strong>???</strong></span>)</span>.</p>
</div>
<div id="articulatory-suppression-effects" class="section level4">
<h4><span class="header-section-number">7.2.4.2</span> Articulatory suppression effects</h4>
<p>Data were analysed in the same fashion as in the first part of the experiment, using <em>Session</em> (2 modalities, before and after motor activity, contrast-coded) as a within-subject categorical predictor, and <em>Condition</em> (2 modalities, Mouthing and Tapping) as a between-subject categorical predictor and <em>RUM</em> as a dependent variable, in a MLM.</p>
</div>
</div>
</div>
<div id="results-1" class="section level2">
<h2><span class="header-section-number">7.3</span> Results</h2>
<div id="correlation-matrix-between-main-predictors-and-control-variables" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Correlation matrix between main predictors and control variables</h3>
<p>In order to prevent multicollinearity, we estimated the correlation between each pair of continuous predictors. Figure <a href="articulatory-suppression-effects-on-induced-rumination.html#fig:correxp1">7.2</a> displays these correlations along with the marginal distribution of each variable. The absence of strong correlations (<span class="math inline">\(r &gt; 0.8\)</span>) between any of these variables suggests that they can each be included as control variables in the following statistical models. Summary statistics (mean and standard deviation) for all these variables can be found in Table <a href="articulatory-suppression-effects-on-induced-rumination.html#tab:sumstat">7.1</a>.</p>
<div class="figure"><span id="fig:correxp1"></span>
<img src="06-chap6_files/figure-html/correxp1-1.png" alt="Diagonal: marginal distribution of each variable. Panels above the diagonal: Pearson's correlations between main continuous predictors, along with 95% CIs. The absolute size of the correlation coefficient is represented by the size of the text (lower coefficients appear as smaller). Panels below the diagonal: scatterplot of each variables pair." width="960" />
<p class="caption">
Figure 7.2: Diagonal: marginal distribution of each variable. Panels above the diagonal: Pearson’s correlations between main continuous predictors, along with 95% CIs. The absolute size of the correlation coefficient is represented by the size of the text (lower coefficients appear as smaller). Panels below the diagonal: scatterplot of each variables pair.
</p>
</div>
<table>
<caption><span id="tab:sumstat">Table 7.1: </span>Descriptive statistics (mean and standard deviation) of each recorded variable, for the final sample of participants that were included in the study.</caption>
<thead>
<tr class="header">
<th align="left">Variables</th>
<th align="center">Baseline</th>
<th align="center">Post-induction</th>
<th align="center">Post-motor</th>
<th align="center">Baseline</th>
<th align="center">Post-induction</th>
<th align="center">Post-motor</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">RUM</td>
<td align="center">28.5 (26.49)</td>
<td align="center">54.66 (25.16)</td>
<td align="center">45.47 (27.25)</td>
<td align="center">20.96 (21.82)</td>
<td align="center">46.77 (25.74)</td>
<td align="center">43.54 (29.57)</td>
</tr>
<tr class="even">
<td align="left">Age</td>
<td align="center">20.3 (2.65)</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">20.31 (2.53)</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr class="odd">
<td align="left">PANASneg</td>
<td align="center">15.65 (5.67)</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">15.46 (5.08)</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr class="even">
<td align="left">PANASpos</td>
<td align="center">30.91 (4.48)</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">31.25 (4.4)</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr class="odd">
<td align="left">RRSbrooding</td>
<td align="center">12.2 (2.43)</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">12.06 (2.62)</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr class="even">
<td align="left">RRSreflection</td>
<td align="center">12.22 (3.22)</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">11.71 (3.26)</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr class="odd">
<td align="left">Valence</td>
<td align="center">23.56 (22.4)</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">37.53 (24.61)</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr class="even">
<td align="left">Verbality</td>
<td align="center">1.67 (1.18)</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">1.67 (1.26)</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr class="odd">
<td align="left">Vividness</td>
<td align="center">54.17 (28.94)</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">59.78 (24.63)</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
</tbody>
</table>
</div>
<div id="rumination-induction-2" class="section level3">
<h3><span class="header-section-number">7.3.2</span> Rumination induction</h3>
<p>To examine the efficiency of the induction procedure (i.e., the effect of <em>Induction</em>) while controlling for the other variables (i.e., <em>Vividness</em>, <em>RRSbrooding</em>, <em>RRSreflection</em>, <em>PANASpos</em>, and <em>PANASneg</em>), we then compared the parsimony of models containing main constant effects and a varying intercept for <em>Participant</em>. Model comparison showed that the best model (in the sense of the lowest AICc model) was the model including <em>Induction</em>, <em>PANASpos</em>, <em>PANASneg</em>, <em>RRSbooding</em>, and an interaction term between <em>Induction</em> and <em>Vividness</em> as predictors (see Table <a href="articulatory-suppression-effects-on-induced-rumination.html#tab:compexp1">7.2</a>). Fit of the best model was moderate as marginal <span class="math inline">\(R^{2}\)</span> was of 0.3801215 while conditional <span class="math inline">\(R^{2}\)</span> was of 0.6838429.</p>
<table>
<caption><span id="tab:compexp1">Table 7.2: </span>Comparison of models, ordered by AICc relative to the model with the lowest AICc.</caption>
<thead>
<tr class="header">
<th></th>
<th align="left"><span class="math inline">\(K\)</span></th>
<th align="center"><span class="math inline">\(AICc\)</span></th>
<th align="center"><span class="math inline">\(\Delta_{AICc}\)</span></th>
<th align="center"><span class="math inline">\(Weight\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(Int+Ind+PANASpos+PANASneg+Ind:Viv+RRSbro\)</span></td>
<td align="left">8</td>
<td align="center">1903</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(Int+Ind+PANASpos+PANASneg+Ind:Viv+RRSref\)</span></td>
<td align="left">8</td>
<td align="center">1903</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(Int+Ind+PANASpos+PANASneg+Ind:Viv+RRSbro+RRSref\)</span></td>
<td align="left">9</td>
<td align="center">1904</td>
<td align="center">1</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(Int+Ind+PANASneg+Ind:Viv\)</span></td>
<td align="left">6</td>
<td align="center">1914</td>
<td align="center">11</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(Int+Ind+PANASneg\)</span></td>
<td align="left">5</td>
<td align="center">1916</td>
<td align="center">12</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(Int+Ind+PANASpos+Ind:Viv\)</span></td>
<td align="left">6</td>
<td align="center">1917</td>
<td align="center">14</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(Int+Ind+PANASpos\)</span></td>
<td align="left">5</td>
<td align="center">1919</td>
<td align="center">15</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(Int+Ind+Ind:Viv\)</span></td>
<td align="left">5</td>
<td align="center">1928</td>
<td align="center">25</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(Int+Ind\)</span></td>
<td align="left">4</td>
<td align="center">1930</td>
<td align="center">27</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<p>Constant effect estimates for the best model are reported in Table <a href="articulatory-suppression-effects-on-induced-rumination.html#tab:paramexp1">7.3</a>. Based on these values, it seems that <em>Induction</em> (i.e., the effects of the rumination induction) increased <em>RUM</em> scores by approximately 26 points in average (<span class="math inline">\(d_{av} =\)</span> 1.037, 95% CI [0.748, 1.325]). The main negative effect of <em>PANASneg</em> and the main positive effects of <em>PANASpos</em> indicate, respectively, that negative baseline mood was associated with higher levels of rumination while positive baseline mood was associated with lower levels of self-reported rumination.</p>
<table>
<caption><span id="tab:paramexp1">Table 7.3: </span>Coefficient estimates (Est), standard errors (SE), and 95% CIs (Lower, Upper).</caption>
<thead>
<tr class="header">
<th></th>
<th align="center">Est</th>
<th align="center">SE</th>
<th align="center">Lower</th>
<th align="center">Upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td align="center">37.796</td>
<td align="center">1.859</td>
<td align="center">34.153</td>
<td align="center">41.438</td>
</tr>
<tr class="even">
<td>Induction</td>
<td align="center">25.986</td>
<td align="center">2.175</td>
<td align="center">21.724</td>
<td align="center">30.249</td>
</tr>
<tr class="odd">
<td>PANASpos</td>
<td align="center">-6.805</td>
<td align="center">1.877</td>
<td align="center">-10.485</td>
<td align="center">-3.126</td>
</tr>
<tr class="even">
<td>PANASneg</td>
<td align="center">6.995</td>
<td align="center">1.986</td>
<td align="center">3.104</td>
<td align="center">10.887</td>
</tr>
<tr class="odd">
<td>RRSbrooding</td>
<td align="center">2.688</td>
<td align="center">1.997</td>
<td align="center">-1.225</td>
<td align="center">6.601</td>
</tr>
<tr class="even">
<td>Induction:Vividness</td>
<td align="center">4.273</td>
<td align="center">2.178</td>
<td align="center">0.003</td>
<td align="center">8.542</td>
</tr>
</tbody>
</table>
<p>Higher scores on <em>Vividness</em> were associated with higher increase in self-reported rumination after induction, as revealed by the positive coefficient of the interaction term. This suggests that participants who recalled a more vivid negative memory tended to show a higher increase in rumination after the induction procedure than participants with a less vivid memory.</p>
</div>
</div>
<div id="articulatory-suppression-effects-on-induced-rumination-1" class="section level2">
<h2><span class="header-section-number">7.4</span> Articulatory suppression effects on induced rumination</h2>
<p>We then examined the effect of the two motor tasks (articulatory suppression and finger-tapping) on <em>RUM</em>, while controlling for other variables (i.e., <em>Vividness</em>, <em>RRSbrooding</em>, <em>RRSreflection</em>, <em>Verbality</em>, <em>PANASpos</em>, and <em>PANASneg</em>). Given the group differences on <em>RUM</em> score at baseline (i.e., after training), we also included this score as a control variable in our models, as the <em>RUMb</em> variable. Based on our hypotheses, we expected that the model comparison would reveal a three-way interaction between <em>Session</em>, <em>Condition</em> and <em>Verbality</em>. However, the best model identified by AICc model comparison did not include this interaction as a constant effect. Nonetheless, the best model was only slightly better than the model including the three-way interaction (the second model in Table <a href="articulatory-suppression-effects-on-induced-rumination.html#tab:compexp2">7.4</a>), as the best model was only 1.1704261 more <em>credible</em> than the interaction model. As our goal is precise estimation of effects rather than dichotomic decision about the presence or absence of an effect, we chose to present the estimations of the second model as well. Fit of this model was moderate as marginal <span class="math inline">\(R^{2}\)</span> was 0.2928069 while conditional <span class="math inline">\(R^{2}\)</span> was 0.6626111.</p>
<table>
<caption><span id="tab:compexp2">Table 7.4: </span>Comparison of models, ordered by AICc relative to the model with the lowest AICc.</caption>
<thead>
<tr class="header">
<th></th>
<th align="left"><span class="math inline">\(K\)</span></th>
<th align="center"><span class="math inline">\(AICc\)</span></th>
<th align="center"><span class="math inline">\(\Delta_{AICc}\)</span></th>
<th align="center"><span class="math inline">\(Weight\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(Int+Session+Cond+RUMb+PANASneg+RRSbro+RRSref\)</span></td>
<td align="left">9</td>
<td align="center">1914</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(Int+Session+Cond+Session:Cond+RUMb+PANASneg+RRSbro+RRSref\)</span></td>
<td align="left">10</td>
<td align="center">1914</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(Int+Session+Cond+Session:Cond+Session:Cond:Verb+RUMb+PANASneg+RRSbro+RRSref\)</span></td>
<td align="left">11</td>
<td align="center">1916</td>
<td align="center">3</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(Int+Session\)</span></td>
<td align="left">4</td>
<td align="center">1947</td>
<td align="center">33</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(Int+Session+Cond\)</span></td>
<td align="left">5</td>
<td align="center">1948</td>
<td align="center">34</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(Int+Session+Cond+Session:Cond\)</span></td>
<td align="left">6</td>
<td align="center">1948</td>
<td align="center">34</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(Int+Session+Cond+Session:Cond:Verb\)</span></td>
<td align="left">7</td>
<td align="center">1950</td>
<td align="center">36</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(Int\)</span></td>
<td align="left">3</td>
<td align="center">1953</td>
<td align="center">39</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<p>Parameter values of the best model for the second part of the experiment are reported in Table <a href="articulatory-suppression-effects-on-induced-rumination.html#tab:paramexp2">7.5</a>. Based on these values, it seems that self-reported rumination decreased after both motor tasks (the coefficient for <em>Session</em> is negative), but this decrease was substantially larger in the <em>Mouthing</em> condition (<span class="math inline">\(d_{av} =\)</span> -0.351, 95% CI [-0.735, 0.034]) than in the <em>Tapping</em> condition (<span class="math inline">\(d_{av} =\)</span> -0.117, 95% CI [-0.506, 0.273]), as can be read from the coefficient of the interaction term between <em>Session</em> and <em>Condition</em> (<em>Est</em> = 5.965, <em>SE</em> = 4.320, <em>95% CI</em> [-2.502, 14.433]). However, the large uncertainty associated with this result (as expressed by the width of the confidence interval) warrants a careful interpretation of this result, that should be considered as suggestive evidence, rather than conclusive evidence.</p>
<p>The large variation between participants can be appreciated by computing the <em>intra-class correlation</em> (ICC), expressed as <span class="math inline">\(\sigma_{intercept}^{2}/(\sigma_{intercept}^{2}+\sigma_{residuals}^{2})\)</span>. For the best model, the ICC is equal to 0.5229, indicating that 52.29% of the variance in the outcome that remains after accounting for the effect of the predictors, is attributable to systematic inter-individual differences.</p>
<table>
<caption><span id="tab:paramexp2">Table 7.5: </span>Estimates (Est), standard errors (SE), and 95% CIs (Lower, Upper).</caption>
<thead>
<tr class="header">
<th></th>
<th align="center">Est</th>
<th align="center">SE</th>
<th align="center">Lower</th>
<th align="center">Upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td align="center">47.645</td>
<td align="center">1.930</td>
<td align="center">43.862</td>
<td align="center">51.427</td>
</tr>
<tr class="even">
<td>Session</td>
<td align="center">-6.214</td>
<td align="center">2.160</td>
<td align="center">-10.448</td>
<td align="center">-1.980</td>
</tr>
<tr class="odd">
<td>Condition</td>
<td align="center">-0.948</td>
<td align="center">3.921</td>
<td align="center">-8.633</td>
<td align="center">6.736</td>
</tr>
<tr class="even">
<td>RUMbaseline</td>
<td align="center">13.394</td>
<td align="center">2.205</td>
<td align="center">9.073</td>
<td align="center">17.716</td>
</tr>
<tr class="odd">
<td>RRSbrooding</td>
<td align="center">2.449</td>
<td align="center">2.088</td>
<td align="center">-1.643</td>
<td align="center">6.541</td>
</tr>
<tr class="even">
<td>RRSreflection</td>
<td align="center">-2.039</td>
<td align="center">1.981</td>
<td align="center">-5.921</td>
<td align="center">1.843</td>
</tr>
<tr class="odd">
<td>PANASneg</td>
<td align="center">0.300</td>
<td align="center">2.245</td>
<td align="center">-4.100</td>
<td align="center">4.700</td>
</tr>
<tr class="even">
<td><a href="Session:Condition" class="uri">Session:Condition</a></td>
<td align="center">5.965</td>
<td align="center">4.320</td>
<td align="center">-2.502</td>
<td align="center">14.433</td>
</tr>
<tr class="odd">
<td>sd_(Intercept).Participant</td>
<td align="center">16.462</td>
<td align="center">NA</td>
<td align="center">NA</td>
<td align="center">NA</td>
</tr>
<tr class="even">
<td>sd_Observation.Residual</td>
<td align="center">15.724</td>
<td align="center">NA</td>
<td align="center">NA</td>
<td align="center">NA</td>
</tr>
</tbody>
</table>
<p>Figure <a href="articulatory-suppression-effects-on-induced-rumination.html#fig:plotexp1">7.3</a> shows the evolution of the mean <em>RUM</em> scores all through the experiment according to each session (Baseline, Post-induction, Post-motor) and <em>Condition</em> (Mouthing, Tapping). This figure reveals important inter-individual variability, in all conditions. After the rumination induction, <em>RUM</em> score increased in both groups, and decreased after the motor task, with a stronger decrease in the <em>Mouthing</em> condition.</p>
<div class="figure"><span id="fig:plotexp1"></span>
<img src="06-chap6_files/figure-html/plotexp1-1.png" alt="Mean RUM score by Session and Condition, along with violin plots and individual data. Error bars represent 95% CIs. The horizontal bar inside the violin plots represents the median of the conditional distribution." width="768" />
<p class="caption">
Figure 7.3: Mean RUM score by Session and Condition, along with violin plots and individual data. Error bars represent 95% CIs. The horizontal bar inside the violin plots represents the median of the conditional distribution.
</p>
</div>
<p>Figure <a href="articulatory-suppression-effects-on-induced-rumination.html#fig:plotverbal">7.4</a> shows the effects of <em>Verbality</em> on the relative change (i.e., after - before) in self-reported rumination after both motor activities (i.e., <em>Mouthing</em> and <em>Tapping</em>). As <em>Verbality</em> was centered before analysis, its score cannot be interpreted in absolute terms. However, a high score on this index indicates more verbal than non-verbal (e.g., visual images, non-speech sounds) thoughts, while a low score indicates more non-verbal than verbal thoughts. Contrary to our predictions but consistent with the model comparison, this figure depicts a similar relationship between <em>Verbality</em> and the change in <em>RUM</em> score (between before and after the motor task), according to the Condition.</p>
<div class="figure"><span id="fig:plotverbal"></span>
<img src="06-chap6_files/figure-html/plotverbal-1.png" alt="Mean RUM relative change after motor activity, as a function of the degree of Verbality, in the mouthing (the dark grey dots and regression line) and finger tapping (the light grey dots and regression line) conditions." width="768" />
<p class="caption">
Figure 7.4: Mean RUM relative change after motor activity, as a function of the degree of Verbality, in the mouthing (the dark grey dots and regression line) and finger tapping (the light grey dots and regression line) conditions.
</p>
</div>
</div>
<div id="discussion-1" class="section level2">
<h2><span class="header-section-number">7.5</span> Discussion</h2>
<p>The purpose of the current study was to investigate the effects of articulatory suppression on induced verbal rumination. We predicted that if verbal rumination, which can be construed as a type of inner speech, does involve the mental simulation of overt speech production, its generation should be disrupted by articulatory suppression, but not by finger tapping. This prediction was not strictly corroborated by the data, as we observed a decrease of self-reported rumination after both types of motor activities (see Figure <a href="articulatory-suppression-effects-on-induced-rumination.html#fig:plotexp1">7.3</a> and Table <a href="articulatory-suppression-effects-on-induced-rumination.html#tab:compexp2">7.4</a>), with a somewhat stronger decrease in the Mouthing condition. In the following, we examine the validity of our methods and discuss interpretations of our results. Finally, we formulate how subsequent research should address this kind of question and suggest alternative ways to test the above mentioned hypothesis. We begin by discussing the results of the rumination induction procedure.</p>
<div id="rumination-induction-3" class="section level3">
<h3><span class="header-section-number">7.5.1</span> Rumination induction</h3>
<p>It is noteworthy that 32.91% of the total sample of participants who were recruited did not respond to this induction, and were therefore not included in the analyses. Moreover, as reported in Table <a href="articulatory-suppression-effects-on-induced-rumination.html#tab:paramexp1">7.3</a>, it seems that the <em>Vividness</em> of the memory chosen by the participant during the mood induction was moderating the effect of the rumination induction. In other words, the more vivid (i.e., the more “intense”) the memory, the more successful the rumination induction was. This highlights the fact that this aspect should be carefully controlled each time a mood induction is used in order to foster subsequent repetitive negative thinking.</p>
<p>Moreover, we observed a group difference of approximately 7.5 points in the average <em>RUM</em> score at baseline. This difference might be explained by motor training, which took place before baseline measurement of state rumination. During this training, participants had to perform the motor task (either finger-tapping or mouthing) in front of a screen on which a white dot was moving randomly on a black screen, for 1 min. During the task, the experimenter stayed in the room (out of the participant’s sight) to check that participants were performing the motor task adequately. Being an unusual and potentially embarrassing motor activity, mouthing might have been an higher source of stress for the participants, as compared to the more common activity of finger-tapping. This group difference in baseline state rumination subsisted after the induction, as the group difference after the induction is of approximately 8 points (see full dataset and summary statistics in the <a href="articulatory-suppression-effects-on-induced-rumination.html#supp">supplementary materials</a>).</p>
</div>
<div id="articulatory-suppression-effects-1" class="section level3">
<h3><span class="header-section-number">7.5.2</span> Articulatory suppression effects</h3>
<p>In the following section, we discuss in more depth the results of the second part of the study, which aimed at comparing the effects of articulatory suppression and finger-tapping on self-reported rumination.</p>
<p>First, it is important to examine whether our failure to detect the predicted interaction could come from a lack of statistical power. We planned 128 participants in order to reach a power of .80 for a targeted effect size of <span class="math inline">\(\eta_{p}^{2}=.06\)</span>. As explained above, out of the 184 recruited participants, only 106 could be included in the study. With 106 participants, the a priori power was approximately of .70, which is much higher than the median power in typical psychological studies.</p>
<p>Second, it is important to acknowledge that despite the absence of the predicted difference between the two conditions in their influence on the level of self-reported rumination (i.e., <em>RUM</em>), both activities did lead, on average, to a decrease in self-reported rumination of approximately 6 points on the VAS (as indicated by the slope for <em>Session</em> in Table <a href="articulatory-suppression-effects-on-induced-rumination.html#tab:paramexp2">7.5</a>). This decrease might be interpreted in at least two ways. First, it might be explained by the simple exposition to the VAS and by compliance effects. When asked to rate their level of rumination again after five minutes of motor activity, some participants might be prompted to indicate a lower level of rumination than before the motor task. But compliance effects could similarly lead participants to consider the motor task as irritating, and therefore as prone to rumination increase. Some participants could therefore also be biased towards indicating a higher level of rumination after the motor task. Second, it might be considered that this decrease reflects a genuine decrease in rumination. In the following, we adopt the latter perspective and discuss explanations for the weak difference between the two conditions.</p>
<div id="effect-of-the-rumination-quality-verbality" class="section level4">
<h4><span class="header-section-number">7.5.2.1</span> Effect of the rumination quality (verbality)</h4>
<p>Our prediction was that rumination in verbal form would be more disrupted by mouthing than rumination in non-verbal form, while both kinds of rumination would not be disrupted (or similarly disrupted) by finger-tapping. In other words, we hypothesised a three-way interaction, between the effect of time (i.e., <em>Session</em>), <em>Condition</em>, and <em>Verbality</em>. In the following, we discuss the absence of this interaction. Then, we focus on the weak difference between the two conditions (omitting <em>Verbality</em>), and discuss some explanations for this weak difference.</p>
<p>First, the absence of the three-way interaction might come from a difficulty for the participants to have clear introspective access to the ruminative thoughts they experienced during the experiment. For instance, we know that introspective description of inner speech differs considerably, between people trained to regularly report on their episodes of inner speaking, and people without such training <span class="citation">(e.g., <span class="citeproc-not-found" data-reference-id="Hurlburt2013"><strong>???</strong></span>)</span>. Moreover, as the <em>Verbality</em> questionnaire was presented at the end of the experiment, one cannot exclude that it was partly contaminated by recall, which, when done verbally, has been shown to artificially increase the subjective verbality index <span class="citation">(<span class="citeproc-not-found" data-reference-id="Hurlburt2011"><strong>???</strong></span>)</span>.</p>
</div>
<div id="difference-between-motor-conditions" class="section level4">
<h4><span class="header-section-number">7.5.2.2</span> Difference between motor conditions</h4>
<p>Leaving the self-reported quality of rumination aside, we now turn to a discussion of the weak difference between the two conditions. We think this result can be explained in at least two non-exclusive ways. First, we could argue that the decrease observed in both conditions was due to an unexpected effect of finger-tapping on rumination. Second, we could argue that the effect of the articulatory suppression was somehow weaker than expected. In the following, we provide arguments and explanations for each of these possibilities.</p>
<p>Steady finger-tapping is usually considered as a relevant control condition for evaluating articulatory suppression, since it specifically recruits the hand motor system and should not interfere with the oral motor system, while being comparable in terms of general attentional demands <span class="citation">(e.g., <span class="citeproc-not-found" data-reference-id="Gruber2001"><strong>???</strong></span>, <span class="citeproc-no-output"><strong>???</strong></span>)</span>. However, using more complex rhythmic patterns of finger-tapping, <span class="citation">(<span class="citeproc-not-found" data-reference-id="Saito1994"><strong>???</strong></span>)</span> observed a fade-out of the phonological similarity effect in a verbal memory task with spoken recall, when subjects were asked to tap with either their right (dominant) or left hand, while the phonological similarity effect was conserved in the control condition (no tapping). The author concluded that a complex rhythmic tapping task can suppress the activity of the articulatory control process, by <em>suppressing</em> the running of speech motor programs <span class="citation">(<span class="citeproc-not-found" data-reference-id="Saito1994"><strong>???</strong></span>)</span>. More specifically, he suggested that complex, non-automatised, rhythmic finger tapping could use speech motor programs, which are useful to control speech prosody, and therefore can deal with rhythmic activity. We further suggest that a novel complex rhythmic task might require silent verbalisation and, therefore, might itself be an articulatory suppression task. In line with these findings, another study showed that for right-handed subjects, tapping with a finger of the right hand is more effective at interfering with performance of a verbal memory task than is tapping with a finger of the left hand <span class="citation">(<span class="citeproc-not-found" data-reference-id="Friedman1988"><strong>???</strong></span>)</span>. Although Friedman et al.’s findings are difficult to interpret, because task priority was manipulated and this may have led to conflict resolution, which might have been dealt with differentially according to the hand involved, they do suggest that a finger tapping task is not always the best control for articulatory suppression. This might explain the decrease of self-reported rumination observed in our own study, after the finger-tapping, and suggests that we might observe different results by asking participants to tap with the finger of their non-dominant hand. We think it is important to note for future studies that our results, together with those of <span class="citation">(<span class="citeproc-not-found" data-reference-id="Saito1994"><strong>???</strong></span>)</span> and <span class="citation">(<span class="citeproc-not-found" data-reference-id="Friedman1988"><strong>???</strong></span>)</span>, suggest that finger-tapping could in fact interfere with inner speech. In other words, finger-tapping, with the dominant hand, is probably not an appropriate control condition when studying articulatory suppression.</p>
<p>As suggested previously, an alternative way to explain the absence of differences between the two motor conditions is to suppose that the effects of the articulatory suppression were weaker than we expected. The rhythmic mouthing task might have become too automatised to disrupt inner speech programming. This idea finds some support in the results of <span class="citation">(<span class="citeproc-not-found" data-reference-id="Saito1997"><strong>???</strong></span>)</span>, who observed an effect of articulatory suppression on the phonological similarity effect in a memory task only when the articulatory suppression was <em>intermittent</em> (i.e., “ah, ah, ah…”) but no effect when participants had to utter a continuous “ah–”. This can be explained by considering that the intermittent articulatory suppression would impose a greater load on speech motor programming than the continuous articulatory suppression <span class="citation">(<span class="citeproc-not-found" data-reference-id="Saito1997"><strong>???</strong></span>)</span>. In a similar vein, <span class="citation">(<span class="citeproc-not-found" data-reference-id="Macken1995"><strong>???</strong></span>)</span> found stronger effects of articulatory suppression when participants were asked to repeat a sequence of different letters than when they were asked to repeat a single letter. One way to examine this hypothesis with our own protocol would be to ask participants to make sequences of various mouth movements, rather than repeating a single movement.</p>
<p>In a broader perspective, relating to the original research question, we should mention two additional interpretations of our results. So far, we considered different ways to explain either how the finger-tapping task could interfere with rumination or how the articulatory suppression task might have failed to disrupt rumination. However, if we assume that our scales (especially the <em>RUM</em> outcome response and the <em>Verbality</em> scale) are reliable and that the articulatory suppression was efficient in its intended purpose, we are forced to admit that either i) rumination is not a type of inner speech that can be disrupted by peripheral muscle perturbation (i.e., it could be described as a more abstract form of inner speech) or that ii) inner speech, more broadly, does not depend on peripheral speech muscle activity. Although we think that these questions cannot be answered from our present results, we acknowledge that these two possibilities are compatible with our results.</p>
<p>In summary, the current research is one of the first behavioral studies exploring the association between verbal rumination and the speech motor system. While the observed data did not strictly corroborate our original hypotheses, we explored several explanations for the weak difference between articulatory suppression and the control task, and related our findings to previous works on the role of inner speech in verbal working memory. These results have important implications for future studies on articulatory suppression during inner speech or working memory tasks. More precisely, they highlight the need for further investigation of the most appropriate control task when studying the effects of articulatory suppression.</p>
</div>
</div>
</div>
<div id="supp" class="section level2">
<h2><span class="header-section-number">7.6</span> Supplementary materials</h2>
<p>Pre-registered protocol, preprint, data, as well as reproducible code and figures are available at: <a href="http://osf.io/3bh67">osf.io/3bh67</a>.</p>
<p>A lot of useful packages have been used for the writing of this paper, among which the <code>papaja</code> and <code>knitr</code> packages for writing and formatting <span class="citation">(Aust &amp; Barth, <a href="#ref-R-papaja">2018</a>; Xie, <a href="#ref-R-knitr">2018</a>)</span>, the <code>ggplot2</code>, <code>ggforce</code>, <code>GGally</code>, <code>DiagrammeR</code>, and <code>plotly</code> packages for plotting <span class="citation">(Iannone, <a href="#ref-R-DiagrammeR">2018</a>; Pedersen, <a href="#ref-R-ggforce">2018</a>; Schloerke et al., <a href="#ref-R-GGally">2018</a>; Sievert et al., <a href="#ref-R-plotly">2017</a>; Wickham et al., <a href="#ref-R-ggplot2">2018</a>)</span>, the <code>AICcmodavg</code>, and <code>Rmisc</code> packages for data analysis <span class="citation">(Hope, <a href="#ref-R-Rmisc">2013</a>; Mazerolle., <a href="#ref-R-AICcmodavg">2017</a>)</span>, as well as the <code>tidyverse</code> and <code>broom</code> packages for code writing and formatting <span class="citation">(Robinson, <a href="#ref-R-broom">2018</a>; Wickham, <a href="#ref-R-tidyverse">2017</a>)</span>.</p>
</div>
<div id="acknowledgements-1" class="section level2">
<h2><span class="header-section-number">7.7</span> Acknowledgements</h2>
<p>This project was funded by the ANR project INNERSPEECH (grant number ANR-13-BSH2-0003-01). The first author of the manuscript is funded by a fellowship from Université Grenoble Alpes. We thank David Meary for his technical support in programming the eye-tracking experiment. We thank Rafael Laboissiere and Brice Beffara for their advice concerning data analysis.</p>
</div>
<div id="appendix-a.-eye-tracking-control-experiment" class="section level2">
<h2><span class="header-section-number">7.8</span> Appendix A. Eye-tracking control experiment</h2>
<p>The purpose of this control experiment was to demonstrate that the two motor tasks used in the main experiment, namely, finger tapping and articulatory suppression (mouth movements) were equivalent in terms of task difficulty or general dual-task demand <span class="citation">(<span class="citeproc-not-found" data-reference-id="Emerson2003"><strong>???</strong></span>)</span>. Participants performed a computer-based visual search task (i.e., finding a T among an array of Ls), adapted from the <span class="citation">(<span class="citeproc-not-found" data-reference-id="Treisman1980"><strong>???</strong></span>)</span> paradigm (see below for details).</p>
<div id="sample-1" class="section level3">
<h3><span class="header-section-number">7.8.1</span> Sample</h3>
<p>Twenty-four participants (Mean age = 19.46, SD = 1.18, Min-Max = 18-21, 21 females, 21 right-handed), drawn from the same population (i.e., undergraduate psychology students) as the main experiment took part in this eye-tracking pretest.</p>
</div>
<div id="sample-size" class="section level3">
<h3><span class="header-section-number">7.8.2</span> Sample size</h3>
<p>As we aimed to compare four conditions (i.e., visual search (VS) task alone, VS + finger tapping, VS + foot tapping and VS + mouth movements), we recruited 24 participants in order to have at least one participant per order in our random counter-balanced repeated measures design (<span class="math inline">\(n = k!\)</span> where <span class="math inline">\(n\)</span> is the number of possible orders of conditions for <span class="math inline">\(k\)</span> conditions, then <span class="math inline">\(n =4 != 24\)</span>).</p>
</div>
<div id="material-2" class="section level3">
<h3><span class="header-section-number">7.8.3</span> Material</h3>
<p>Experiment took place individually in a dark room. Participants had to seat in front of a 22 inches, Iyama Vision Master Pro 513-MA203DT CRT Monitor (resolution: 1024x768 pixels, refresh rate: 85 Hz) with a NVIDIA GeForce 9800 GTX+ graphic processor. A camera-based eye-tracker (EyeLink 1000 from SR Research) with a sampling rate of 250 Hz and a minimum accuracy of 0.5° was used, in the pupil-corneal reflection tracking mode. Participants were positioned on a seat so as to keep distance from the camera to the forehead target between 50 and 60 cm. A five-point calibration was completed before presenting stimuli, at the beginning of each condition.</p>
</div>
<div id="procedure-2" class="section level3">
<h3><span class="header-section-number">7.8.4</span> Procedure</h3>
<p>The target (i.e., the letter “T”) was present at each trial, either on the right or on the left of the central vertical axis of the grid. The grid was an array of 6*6 items. Each stimulus was displayed until the participant response (maximum duration in case of no response: 5 seconds). Each grid of letters was preceded by a central fixation circle, that was displayed for 500ms after the participant moved his/her gaze towards it. In order to give their response (“left” or “right”), participants had to gaze towards a large filled gray circle, situated either on the left or on the right side of the grid. Each participant went through each condition, in a random order. A first general training session was proposed, at the beginning of the experiment, using ten items that were not used subsequently in the four conditions. Each condition was composed of 90 trials (45 left and 45 right), knowing that the first ten trials of each condition were considered as training trials and thus not included in analysis. All participants were filmed in order to ensure that they effectively performed the motor activity. Our measure of interest was the delay between the apparition of the grid and the participant’s response (the time at which his/her gaze reached the response circle), below referred to as “response time” (RT).</p>
</div>
<div id="data-preprocessing" class="section level3">
<h3><span class="header-section-number">7.8.5</span> Data preprocessing</h3>
<p>Raw data from EyeLink includes gaze on screen spatial coordinates, pupil diameter and forehead target spatial coordinates, with its distance from the camera. For this experiment, since only RTs (in ms) of correct trials are interesting, invalid trials (when no response has been given) and wrong responses were removed from the analysis.</p>
</div>
<div id="data-analysis-1" class="section level3">
<h3><span class="header-section-number">7.8.6</span> Data analysis</h3>
<p>Data were analysed using <em>Condition</em> (4 modalities) as a within-subject predictor and the natural logarithm of the RT as a dependent variable in a MLM, including a varying intercept for both <em>participant</em> and <em>item</em>. Comparisons of interest were computed using Helmert contrasts. Estimates and confidence intervals are reported for each comparison in the log scale.</p>
</div>
<div id="results-2" class="section level3">
<h3><span class="header-section-number">7.8.7</span> Results</h3>
<p>Results of the MLM are reported in Table <a href="articulatory-suppression-effects-on-induced-rumination.html#tab:paramET">7.6</a> and Figure <a href="articulatory-suppression-effects-on-induced-rumination.html#fig:eyetrack">7.5</a>. Contrast analysis revealed a slight difference between the <em>Control</em> condition and the mean of the three other conditions (Est = -0.0099591, 95% CI = [-0.015235, -0.0046832]) as well as a slight difference between the <em>Foot</em> condition and the mean of the <em>Finger</em> and the <em>Mouth</em> conditions (Est = 0.0050298, 95% CI = [-0.0024395, 0.0124992]) while no apparent differences between the <em>Mouth</em> and the <em>Finger</em> conditions (Est = -0.0063662, 95% CI = [-0.0192938, 0.0065614]).</p>
<table>
<caption><span id="tab:paramET">Table 7.6: </span>Coefficient estimates (Est), standard errors (SE), and 95% CIs (Lower, Upper) of each contrast.</caption>
<thead>
<tr class="header">
<th></th>
<th align="center"><span class="math inline">\(Est\)</span></th>
<th align="center"><span class="math inline">\(SE\)</span></th>
<th align="center"><span class="math inline">\(Lower\)</span></th>
<th align="center"><span class="math inline">\(Upper\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(Control\ vs\ all\)</span></td>
<td align="center">-0.00996</td>
<td align="center">0.00269</td>
<td align="center">-0.01524</td>
<td align="center">-0.00468</td>
</tr>
<tr class="even">
<td><span class="math inline">\(Foot\ vs\ Finger + Mouth\)</span></td>
<td align="center">0.00503</td>
<td align="center">0.00381</td>
<td align="center">-0.00244</td>
<td align="center">0.01250</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(Finger\ vs\ Mouth\)</span></td>
<td align="center">-0.00637</td>
<td align="center">0.00660</td>
<td align="center">-0.01929</td>
<td align="center">0.00656</td>
</tr>
</tbody>
</table>
<div class="figure"><span id="fig:eyetrack"></span>
<img src="06-chap6_files/figure-html/eyetrack-1.png" alt="Mean RTs by Condition along with 95% CIs and violin plots (the horizontal line represents the median of the conditional distribution). Grey dots represent mean RTs by participant." width="768" />
<p class="caption">
Figure 7.5: Mean RTs by Condition along with 95% CIs and violin plots (the horizontal line represents the median of the conditional distribution). Grey dots represent mean RTs by participant.
</p>
</div>
</div>
<div id="discussion-2" class="section level3">
<h3><span class="header-section-number">7.8.8</span> Discussion</h3>
<p>This control experiment shows that there is no apparent difference (or a negligible one) in terms of attentional demand between the two motor tasks used in the main experiment (i.e., finger-tapping and mouthing), although performing a dual motor task (of any type) does seem costly, because of the observed difference between the control condition and the mean of the three others conditions. These results are in line with the results obtained by <span class="citation">(<span class="citeproc-not-found" data-reference-id="Cefidekhanie2014"><strong>???</strong></span>)</span> in their control experiment.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-base">
<p>R Core Team. (2018). <em>R: A language and environment for statistical computing</em>. Vienna, Austria: R Foundation for Statistical Computing. Retrieved from <a href="https://www.R-project.org/" class="uri">https://www.R-project.org/</a></p>
</div>
<div id="ref-R-papaja">
<p>Aust, F., &amp; Barth, M. (2018). <em>Papaja: Prepare reproducible apa journal articles with r markdown</em>. Retrieved from <a href="https://github.com/crsh/papaja" class="uri">https://github.com/crsh/papaja</a></p>
</div>
<div id="ref-R-knitr">
<p>Xie, Y. (2018). <em>Knitr: A general-purpose package for dynamic report generation in r</em>. Retrieved from <a href="https://CRAN.R-project.org/package=knitr" class="uri">https://CRAN.R-project.org/package=knitr</a></p>
</div>
<div id="ref-R-DiagrammeR">
<p>Iannone, R. (2018). <em>DiagrammeR: Graph/network visualization</em>. Retrieved from <a href="https://CRAN.R-project.org/package=DiagrammeR" class="uri">https://CRAN.R-project.org/package=DiagrammeR</a></p>
</div>
<div id="ref-R-ggforce">
<p>Pedersen, T. L. (2018). <em>Ggforce: Accelerating ’ggplot2’</em>. Retrieved from <a href="https://CRAN.R-project.org/package=ggforce" class="uri">https://CRAN.R-project.org/package=ggforce</a></p>
</div>
<div id="ref-R-GGally">
<p>Schloerke, B., Crowley, J., Cook, D., Briatte, F., Marbach, M., Thoen, E., … Larmarange, J. (2018). <em>GGally: Extension to ’ggplot2’</em>. Retrieved from <a href="https://CRAN.R-project.org/package=GGally" class="uri">https://CRAN.R-project.org/package=GGally</a></p>
</div>
<div id="ref-R-plotly">
<p>Sievert, C., Parmer, C., Hocking, T., Chamberlain, S., Ram, K., Corvellec, M., &amp; Despouy, P. (2017). <em>Plotly: Create interactive web graphics via ’plotly.js’</em>. Retrieved from <a href="https://CRAN.R-project.org/package=plotly" class="uri">https://CRAN.R-project.org/package=plotly</a></p>
</div>
<div id="ref-R-ggplot2">
<p>Wickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., &amp; Woo, K. (2018). <em>Ggplot2: Create elegant data visualisations using the grammar of graphics</em>. Retrieved from <a href="https://CRAN.R-project.org/package=ggplot2" class="uri">https://CRAN.R-project.org/package=ggplot2</a></p>
</div>
<div id="ref-R-Rmisc">
<p>Hope, R. M. (2013). <em>Rmisc: Ryan miscellaneous</em>. Retrieved from <a href="https://CRAN.R-project.org/package=Rmisc" class="uri">https://CRAN.R-project.org/package=Rmisc</a></p>
</div>
<div id="ref-R-AICcmodavg">
<p>Mazerolle., M. J. (2017). <em>AICcmodavg: Model selection and multimodel inference based on (q)AIC(c)</em>. Retrieved from <a href="https://CRAN.R-project.org/package=AICcmodavg" class="uri">https://CRAN.R-project.org/package=AICcmodavg</a></p>
</div>
<div id="ref-R-broom">
<p>Robinson, D. (2018). <em>Broom: Convert statistical analysis objects into tidy data frames</em>. Retrieved from <a href="https://CRAN.R-project.org/package=broom" class="uri">https://CRAN.R-project.org/package=broom</a></p>
</div>
<div id="ref-R-tidyverse">
<p>Wickham, H. (2017). <em>Tidyverse: Easily install and load the ’tidyverse’</em>. Retrieved from <a href="https://CRAN.R-project.org/package=tidyverse" class="uri">https://CRAN.R-project.org/package=tidyverse</a></p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="5">
<li id="fn5"><p>This experimental chapter is a manuscript reformatted for the need of this thesis. Source: The manuscript has been submitted to Psychological Research. Pre-registered protocol, preprint, data, as well as reproducible code and figures are available at: <a href="https://osf.io/3bh67/" class="uri">https://osf.io/3bh67/</a>.<a href="articulatory-suppression-effects-on-induced-rumination.html#fnref5">↩</a></p></li>
<li id="fn6"><p>In the original power calculations included in the OSF preregistration platform, we had inadequately specified the effect size in GPower, but we only realised this erroneous specification after the freezing of the preregistration on the OSF platform. Therefore, the current sample size slightly differs from the preregistered one.<a href="articulatory-suppression-effects-on-induced-rumination.html#fnref6">↩</a></p></li>
<li id="fn7"><p>These may be interpreted as tests of significance: if the confidence interval for an estimated parameter does not contain zero, this estimate may be considered significant at <span class="math inline">\(\alpha\)</span> &lt;.05.<a href="articulatory-suppression-effects-on-induced-rumination.html#fnref7">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="zygoto-experiment.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tms-study.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/lnalborczyk/phd_thesis/edit/master/06-chap6.Rmd",
"text": "Edit"
},
"download": ["thesis.pdf"],
"toc": {
"collapse": "none",
"scroll_highlight": true
},
"search": true,
"highlight": "pygments"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
